{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"13zz7eG28eboUyppiBmKG3fxdF3K9lXXd","authorship_tag":"ABX9TyOm05aV8bUyKD+W9Lw06/2P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"UNfWEQQy4EZo","executionInfo":{"status":"ok","timestamp":1677466377555,"user_tz":300,"elapsed":343,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}}},"outputs":[],"source":["# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = '/content/drive/MyDrive/GitHub' \n","GIT_REPOSITORY = \"COS598D-Pruning\" \n","\n","# PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","# PROJECT_PATH = MY_GOOGLE_DRIVE_PATH\n","PROJECT_PATH = f\"/content/drive/MyDrive/GitHub/{GIT_REPOSITORY}\""]},{"cell_type":"code","source":["%cd {PROJECT_PATH}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dMGB5gFR4O39","executionInfo":{"status":"ok","timestamp":1677466379704,"user_tz":300,"elapsed":385,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"f31be3f6-7dab-4387-8c73-b3bc6e3b5413"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/COS598D-Pruning\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rmem9hao4TY-","executionInfo":{"status":"ok","timestamp":1677466380369,"user_tz":300,"elapsed":346,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"573aa2ed-c0a2-436f-f05a-62265206dd03"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mData\u001b[0m/         \u001b[01;34mLayers\u001b[0m/  prune.py      README.md         run_this.ipynb\n","\u001b[01;34mExperiments\u001b[0m/  main.py  \u001b[01;34mPruners\u001b[0m/      requirements.txt  train.py\n","__init__.py   \u001b[01;34mModels\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34mResults\u001b[0m/          \u001b[01;34mUtils\u001b[0m/\n"]}]},{"cell_type":"code","source":["# !pip install timeit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiHLS_a9hweC","executionInfo":{"status":"ok","timestamp":1677452267790,"user_tz":300,"elapsed":2113,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"a3459bbb-c53c-48f6-b421-19014c5e41b1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement timeit (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for timeit\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import timeit"],"metadata":{"id":"IGgbEqK18pIk","executionInfo":{"status":"ok","timestamp":1677454127615,"user_tz":300,"elapsed":344,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjZkWGUF4cFO","executionInfo":{"status":"ok","timestamp":1677466438847,"user_tz":300,"elapsed":56319,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"3b7d0d9b-8298-4dfc-ce80-f3ff58a70ef7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (4.64.1)\n","Collecting argparse\n","  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n","Requirement already satisfied: matplotlib>2.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.5.3)\n","Requirement already satisfied: numpy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (1.3.5)\n","Collecting torch===1.4.0\n","  Downloading torch-1.4.0-cp38-cp38-manylinux1_x86_64.whl (753.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.4/753.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision===0.5.0\n","  Downloading torchvision-0.5.0-cp38-cp38-manylinux1_x86_64.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from torchvision===0.5.0->-r requirements.txt (line 7)) (1.15.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision===0.5.0->-r requirements.txt (line 7)) (7.1.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>2.0.0->-r requirements.txt (line 3)) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>2.0.0->-r requirements.txt (line 3)) (4.38.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>2.0.0->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>2.0.0->-r requirements.txt (line 3)) (23.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>2.0.0->-r requirements.txt (line 3)) (0.11.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>2.0.0->-r requirements.txt (line 3)) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 5)) (2022.7.1)\n","Installing collected packages: argparse, torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.4.0 which is incompatible.\n","fastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\n","fastai 2.7.11 requires torchvision>=0.8.2, but you have torchvision 0.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed argparse-1.4.0 torch-1.4.0 torchvision-0.5.0\n"]}]},{"cell_type":"code","source":["!python main.py --help"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3J8XuEUH4xf0","executionInfo":{"status":"ok","timestamp":1677437754517,"user_tz":300,"elapsed":23048,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"7d13ce19-c7b0-48a3-afac-c6fdada4a8c8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: main.py\n","       [-h]\n","       [--dataset {mnist,cifar10,cifar100,tiny-imagenet,imagenet}]\n","       [--model {fc,conv,vgg11,vgg11-bn,vgg13,vgg13-bn,vgg16,vgg16-bn,vgg19,vgg19-bn,resnet18,resnet20,resnet32,resnet34,resnet44,resnet50,resnet56,resnet101,resnet110,resnet110,resnet152,resnet1202,wide-resnet18,wide-resnet20,wide-resnet32,wide-resnet34,wide-resnet44,wide-resnet50,wide-resnet56,wide-resnet101,wide-resnet110,wide-resnet110,wide-resnet152,wide-resnet1202}]\n","       [--model-class {default,lottery,tinyimagenet,imagenet}]\n","       [--dense-classifier DENSE_CLASSIFIER]\n","       [--pretrained PRETRAINED]\n","       [--optimizer {sgd,momentum,adam,rms}]\n","       [--train-batch-size TRAIN_BATCH_SIZE]\n","       [--test-batch-size TEST_BATCH_SIZE]\n","       [--pre-epochs PRE_EPOCHS]\n","       [--post-epochs POST_EPOCHS]\n","       [--lr LR]\n","       [--lr-drops [LR_DROPS [LR_DROPS ...]]]\n","       [--lr-drop-rate LR_DROP_RATE]\n","       [--weight-decay WEIGHT_DECAY]\n","       [--pruner {rand,mag,snip,grasp,synflow}]\n","       [--compression COMPRESSION]\n","       [--prune-epochs PRUNE_EPOCHS]\n","       [--compression-schedule {linear,exponential}]\n","       [--mask-scope {global,local}]\n","       [--prune-dataset-ratio PRUNE_DATASET_RATIO]\n","       [--prune-batch-size PRUNE_BATCH_SIZE]\n","       [--prune-bias PRUNE_BIAS]\n","       [--prune-batchnorm PRUNE_BATCHNORM]\n","       [--prune-residual PRUNE_RESIDUAL]\n","       [--prune-train-mode PRUNE_TRAIN_MODE]\n","       [--reinitialize REINITIALIZE]\n","       [--shuffle SHUFFLE]\n","       [--invert INVERT]\n","       [--pruner-list [PRUNER_LIST [PRUNER_LIST ...]]]\n","       [--prune-epoch-list [PRUNE_EPOCH_LIST [PRUNE_EPOCH_LIST ...]]]\n","       [--compression-list [COMPRESSION_LIST [COMPRESSION_LIST ...]]]\n","       [--level-list [LEVEL_LIST [LEVEL_LIST ...]]]\n","       [--experiment {singleshot,multishot,unit-conservation,layer-conservation,imp-conservation,schedule-conservation}]\n","       [--expid EXPID]\n","       [--result-dir RESULT_DIR]\n","       [--gpu GPU]\n","       [--workers WORKERS]\n","       [--no-cuda]\n","       [--seed SEED]\n","       [--verbose]\n","\n","Network\n","Compression\n","\n","optional arguments:\n","  -h, --help\n","    show this\n","    help\n","    message and\n","    exit\n","  --experiment {singleshot,multishot,unit-conservation,layer-conservation,imp-conservation,schedule-conservation}\n","    experiment\n","    name\n","    (default:\n","    example)\n","  --expid EXPID\n","    name used\n","    to save\n","    results\n","    (default:\n","    \"\")\n","  --result-dir RESULT_DIR\n","    path to\n","    directory\n","    to save\n","    results\n","    (default: \"\n","    Results/dat\n","    a\")\n","  --gpu GPU\n","    number of\n","    GPU device\n","    to use\n","    (default:\n","    0)\n","  --workers WORKERS\n","    number of\n","    data\n","    loading\n","    workers\n","    (default:\n","    4)\n","  --no-cuda\n","    disables\n","    CUDA\n","    training\n","  --seed SEED\n","    random seed\n","    (default:\n","    1)\n","  --verbose\n","    print\n","    statistics\n","    during\n","    training\n","    and testing\n","\n","training:\n","  --dataset {mnist,cifar10,cifar100,tiny-imagenet,imagenet}\n","    dataset\n","    (default:\n","    mnist)\n","  --model {fc,conv,vgg11,vgg11-bn,vgg13,vgg13-bn,vgg16,vgg16-bn,vgg19,vgg19-bn,resnet18,resnet20,resnet32,resnet34,resnet44,resnet50,resnet56,resnet101,resnet110,resnet110,resnet152,resnet1202,wide-resnet18,wide-resnet20,wide-resnet32,wide-resnet34,wide-resnet44,wide-resnet50,wide-resnet56,wide-resnet101,wide-resnet110,wide-resnet110,wide-resnet152,wide-resnet1202}\n","    model archi\n","    tecture\n","    (default:\n","    fc)\n","  --model-class {default,lottery,tinyimagenet,imagenet}\n","    model class\n","    (default:\n","    default)\n","  --dense-classifier DENSE_CLASSIFIER\n","    ensure last\n","    layer of\n","    model is\n","    dense\n","    (default:\n","    False)\n","  --pretrained PRETRAINED\n","    load\n","    pretrained\n","    weights\n","    (default:\n","    False)\n","  --optimizer {sgd,momentum,adam,rms}\n","    optimizer\n","    (default:\n","    adam)\n","  --train-batch-size TRAIN_BATCH_SIZE\n","    input batch\n","    size for\n","    training\n","    (default:\n","    64)\n","  --test-batch-size TEST_BATCH_SIZE\n","    input batch\n","    size for\n","    testing\n","    (default:\n","    256)\n","  --pre-epochs PRE_EPOCHS\n","    number of\n","    epochs to\n","    train\n","    before\n","    pruning\n","    (default:\n","    0)\n","  --post-epochs POST_EPOCHS\n","    number of\n","    epochs to\n","    train after\n","    pruning\n","    (default:\n","    10)\n","  --lr LR\n","    learning\n","    rate\n","    (default:\n","    0.001)\n","  --lr-drops [LR_DROPS [LR_DROPS ...]]\n","    list of\n","    learning\n","    rate drops\n","    (default:\n","    [])\n","  --lr-drop-rate LR_DROP_RATE\n","    multiplicat\n","    ive factor\n","    of learning\n","    rate drop\n","    (default:\n","    0.1)\n","  --weight-decay WEIGHT_DECAY\n","    weight\n","    decay\n","    (default:\n","    0.0)\n","\n","pruning:\n","  --pruner {rand,mag,snip,grasp,synflow}\n","    prune\n","    strategy\n","    (default:\n","    rand)\n","  --compression COMPRESSION\n","    quotient of\n","    prunable\n","    non-zero\n","    prunable\n","    parameters\n","    before and\n","    after\n","    pruning\n","    (default:\n","    1.0)\n","  --prune-epochs PRUNE_EPOCHS\n","    number of\n","    iterations\n","    for scoring\n","    (default:\n","    1)\n","  --compression-schedule {linear,exponential}\n","    whether to\n","    use a\n","    linear or\n","    exponential\n","    compression\n","    schedule\n","    (default: e\n","    xponential)\n","  --mask-scope {global,local}\n","    masking\n","    scope\n","    (global or\n","    layer)\n","    (default:\n","    global)\n","  --prune-dataset-ratio PRUNE_DATASET_RATIO\n","    ratio of\n","    prune\n","    dataset\n","    size and\n","    number of\n","    classes\n","    (default:\n","    10)\n","  --prune-batch-size PRUNE_BATCH_SIZE\n","    input batch\n","    size for\n","    pruning\n","    (default:\n","    256)\n","  --prune-bias PRUNE_BIAS\n","    whether to\n","    prune bias\n","    parameters\n","    (default:\n","    False)\n","  --prune-batchnorm PRUNE_BATCHNORM\n","    whether to\n","    prune\n","    batchnorm\n","    layers\n","    (default:\n","    False)\n","  --prune-residual PRUNE_RESIDUAL\n","    whether to\n","    prune\n","    residual\n","    connections\n","    (default:\n","    False)\n","  --prune-train-mode PRUNE_TRAIN_MODE\n","    whether to\n","    prune in\n","    train mode\n","    (default:\n","    False)\n","  --reinitialize REINITIALIZE\n","    whether to \n","    reinitializ\n","    e weight\n","    parameters\n","    after\n","    pruning\n","    (default:\n","    False)\n","  --shuffle SHUFFLE\n","    whether to\n","    shuffle\n","    masks after\n","    pruning\n","    (default:\n","    False)\n","  --invert INVERT\n","    whether to\n","    invert\n","    scores\n","    during\n","    pruning\n","    (default:\n","    False)\n","  --pruner-list [PRUNER_LIST [PRUNER_LIST ...]]\n","    list of\n","    pruning\n","    strategies\n","    for\n","    singleshot\n","    (default:\n","    [])\n","  --prune-epoch-list [PRUNE_EPOCH_LIST [PRUNE_EPOCH_LIST ...]]\n","    list of\n","    prune\n","    epochs for\n","    singleshot\n","    (default:\n","    [])\n","  --compression-list [COMPRESSION_LIST [COMPRESSION_LIST ...]]\n","    list of\n","    compression\n","    ratio\n","    exponents\n","    for singles\n","    hot/multish\n","    ot\n","    (default:\n","    [])\n","  --level-list [LEVEL_LIST [LEVEL_LIST ...]]\n","    list of\n","    number of\n","    prune-train\n","    cycles\n","    (levels)\n","    for\n","    multishot\n","    (default:\n","    [])\n"]}]},{"cell_type":"code","source":["# how to run\n","# python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 0.5\n","\n","# --dataset {mnist,cifar10,cifar100,tiny-imagenet,imagenet}"],"metadata":{"id":"YP25T_ij6Idu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you are using Google Colab, to accommodate the limited resources on Google Colab, you could use --pre-epochs 10 for magnitude pruning and use --post-epoch 10 for cifar10 for experiments on Colab. And state the epoch numbers you set in your report."],"metadata":{"id":"FLBCq7Jb7Mla"}},{"cell_type":"markdown","source":["1. Hyper-parameter tuning"],"metadata":{"id":"9DIz2QPV8LuA"}},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner rand --compression 1 --post-epochs 10 --expid cifar_vgg_rand"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stAemLf27NgZ","executionInfo":{"status":"ok","timestamp":1677452673273,"user_tz":300,"elapsed":320560,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"ccbf6983-fc0d-42b3-8680-2601c192e61e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment 'singleshot' with expid 'cifar_vgg_rand' exists.  Overwrite (yes/no)? yes\n","Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.659820587000013\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.44it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:14<00:00, 25.48s/it]\n","Time_post_train:  256.310130729\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302585          10.47          50.00\n","Final      10    2.302675   2.302590          10.00          50.00\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.100694     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.100423    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.100898    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.099765   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.099565   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.100325   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.099257   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.100311  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.100044  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.100143  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.100048  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.099625  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.100114  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.103906     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance    score sum  score abs mean  \\\n","0    -0.027050        1.005513   -46.742134        0.805873   \n","1     0.000000        0.000000     0.000000        0.000000   \n","2     0.000872        0.996963    32.139240        0.796522   \n","3     0.000000        0.000000     0.000000        0.000000   \n","4     0.001659        1.001431   122.280731        0.798236   \n","5     0.000000        0.000000     0.000000        0.000000   \n","6    -0.002633        0.996887  -388.247467        0.796746   \n","7     0.000000        0.000000     0.000000        0.000000   \n","8    -0.001425        1.002429  -420.358032        0.799537   \n","9     0.000000        0.000000     0.000000        0.000000   \n","10    0.001097        1.001136   647.131470        0.798347   \n","11    0.000000        0.000000     0.000000        0.000000   \n","12    0.000697        0.998635   411.355530        0.797115   \n","13    0.000000        0.000000     0.000000        0.000000   \n","14    0.000298        1.001965   351.712585        0.798915   \n","15    0.000000        0.000000     0.000000        0.000000   \n","16    0.000297        1.000542   700.673950        0.797925   \n","17    0.000000        0.000000     0.000000        0.000000   \n","18    0.000261        0.999856   616.478821        0.797893   \n","19    0.000000        0.000000     0.000000        0.000000   \n","20   -0.000339        0.999540  -800.459961        0.797691   \n","21    0.000000        0.000000     0.000000        0.000000   \n","22   -0.000862        0.997625 -2034.541504        0.796859   \n","23    0.000000        0.000000     0.000000        0.000000   \n","24    0.000637        0.999602  1502.357544        0.797581   \n","25    0.000000        0.000000     0.000000        0.000000   \n","26    0.006237        1.002876    31.934721        0.798595   \n","27    0.000000        0.000000     0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.356814   1.392548e+03      True  \n","1             0.000000   0.000000e+00     False  \n","2             0.362516   2.936300e+04      True  \n","3             0.000000   0.000000e+00     False  \n","4             0.364253   5.885236e+04      True  \n","5             0.000000   0.000000e+00     False  \n","6             0.362089   1.174851e+05      True  \n","7             0.000000   0.000000e+00     False  \n","8             0.363171   2.357931e+05      True  \n","9             0.000000   0.000000e+00     False  \n","10            0.363779   4.708842e+05      True  \n","11            0.000000   0.000000e+00     False  \n","12            0.363243   4.701573e+05      True  \n","13            0.000000   0.000000e+00     False  \n","14            0.363700   9.424383e+05      True  \n","15            0.000000   0.000000e+00     False  \n","16            0.363858   1.882542e+06      True  \n","17            0.000000   0.000000e+00     False  \n","18            0.363222   1.882466e+06      True  \n","19            0.000000   0.000000e+00     False  \n","20            0.363228   1.881990e+06      True  \n","21            0.000000   0.000000e+00     False  \n","22            0.362642   1.880025e+06      True  \n","23            0.000000   0.000000e+00     False  \n","24            0.363466   1.881731e+06      True  \n","25            0.000000   0.000000e+00     False  \n","26            0.365161   4.088806e+03      True  \n","27            0.000000   0.000000e+00     False  \n","Parameter Sparsity: 1475792/14719818 (0.1003)\n","FLOP Sparsity: 31608901/313478154 (0.1008)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner mag --compression 1 --post-epochs 10 --pre-epochs 10 --expid cifar_vgg_mag"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ihXMvba82o1","executionInfo":{"status":"ok","timestamp":1677453249862,"user_tz":300,"elapsed":576598,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"dfa2ca43-f9bb-4771-a2ec-1cf0bf40546d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment 'singleshot' with expid 'cifar_vgg_mag' exists.  Overwrite (yes/no)? yes\n","Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [04:31<00:00, 27.17s/it]\n","Time_pre_train:  273.424880113\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00,  6.98it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:31<00:00, 27.15s/it]\n","Time_post_train:  273.15908947799994\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  10    0.636168   0.625882          78.96          98.43\n","Post-Prune 0          NaN   2.198751          28.27          58.58\n","Final      10    0.362002   0.484451          84.35          99.06\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.865162     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.460775    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.466607    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.342041   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.336253   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.242089   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.213114   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.214671  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.106745  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.065246  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.050061  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.044524  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.049265  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.426953     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance     score sum  score abs mean  \\\n","0     0.215746        0.026066    372.808960        0.215746   \n","1     0.000000        0.000000      0.000000        0.000000   \n","2     0.053546        0.001713   1973.927124        0.053546   \n","3     0.000000        0.000000      0.000000        0.000000   \n","4     0.054325        0.001772   4005.279297        0.054325   \n","5     0.000000        0.000000      0.000000        0.000000   \n","6     0.041933        0.001118   6183.344727        0.041933   \n","7     0.000000        0.000000      0.000000        0.000000   \n","8     0.041539        0.001103  12250.351562        0.041539   \n","9     0.000000        0.000000      0.000000        0.000000   \n","10    0.034288        0.000832  20223.839844        0.034288   \n","11    0.000000        0.000000      0.000000        0.000000   \n","12    0.032159        0.000741  18968.369141        0.032159   \n","13    0.000000        0.000000      0.000000        0.000000   \n","14    0.032218        0.000737  38005.714844        0.032218   \n","15    0.000000        0.000000      0.000000        0.000000   \n","16    0.023702        0.000453  55919.343750        0.023702   \n","17    0.000000        0.000000      0.000000        0.000000   \n","18    0.020526        0.000309  48428.031250        0.020526   \n","19    0.000000        0.000000      0.000000        0.000000   \n","20    0.019337        0.000255  45621.121094        0.019337   \n","21    0.000000        0.000000      0.000000        0.000000   \n","22    0.018923        0.000234  44643.882812        0.018923   \n","23    0.000000        0.000000      0.000000        0.000000   \n","24    0.019276        0.000248  45477.464844        0.019276   \n","25    0.000000        0.000000      0.000000        0.000000   \n","26    0.049628        0.001454    254.092865        0.049628   \n","27    0.000000        0.000000      0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.026066     372.808960      True  \n","1             0.000000       0.000000     False  \n","2             0.001713    1973.927124      True  \n","3             0.000000       0.000000     False  \n","4             0.001772    4005.279297      True  \n","5             0.000000       0.000000     False  \n","6             0.001118    6183.344727      True  \n","7             0.000000       0.000000     False  \n","8             0.001103   12250.351562      True  \n","9             0.000000       0.000000     False  \n","10            0.000832   20223.839844      True  \n","11            0.000000       0.000000     False  \n","12            0.000741   18968.369141      True  \n","13            0.000000       0.000000     False  \n","14            0.000737   38005.714844      True  \n","15            0.000000       0.000000     False  \n","16            0.000453   55919.343750      True  \n","17            0.000000       0.000000     False  \n","18            0.000309   48428.031250      True  \n","19            0.000000       0.000000     False  \n","20            0.000255   45621.121094      True  \n","21            0.000000       0.000000     False  \n","22            0.000234   44643.882812      True  \n","23            0.000000       0.000000     False  \n","24            0.000248   45477.464844      True  \n","25            0.000000       0.000000     False  \n","26            0.001454     254.092865      True  \n","27            0.000000       0.000000     False  \n","Parameter Sparsity: 1475793/14719818 (0.1003)\n","FLOP Sparsity: 76353420/313478154 (0.2436)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner snip --compression 1 --post-epochs 10 --expid cifar_vgg_snip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yLnRxXQ88UB","executionInfo":{"status":"ok","timestamp":1677453545819,"user_tz":300,"elapsed":349,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"98319561-2bb1-4d29-9a92-96832e59154c"},"execution_count":9,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7146260500001063\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.74it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:32<00:00, 27.25s/it]\n","Time_post_train:  274.131735404\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.322194           9.76          50.66\n","Final      10    0.656774   0.664631          76.98          98.17\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.858796     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.520616    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.381497    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.252489   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.207747   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.135130   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.146437   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.130799  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.082426  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.082548  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.099391  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.079108  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.080398  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.751758     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   2.148237e-06    8.221934e-12   0.003712    2.148237e-06   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   4.372236e-07    4.510187e-13   0.016118    4.372236e-07   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   3.024112e-07    3.032936e-13   0.022296    3.024112e-07   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   1.802230e-07    1.371213e-13   0.026575    1.802230e-07   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   1.432872e-07    8.967040e-14   0.042257    1.432872e-07   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  8.978097e-08    3.946946e-14   0.052955    8.978097e-08   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  9.579716e-08    4.068194e-14   0.056503    9.579716e-08   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  8.542810e-08    3.210387e-14   0.100775    8.542810e-08   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  5.670231e-08    1.667407e-14   0.133778    5.670231e-08   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  5.675370e-08    1.527773e-14   0.133899    5.675370e-08   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  6.344949e-08    2.087360e-14   0.149696    6.344949e-08   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  5.212994e-08    2.199752e-14   0.122990    5.212994e-08   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  5.339835e-08    2.426836e-14   0.125983    5.339835e-08   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  2.434234e-06    1.646431e-11   0.012463    2.434234e-06   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         8.221934e-12       0.003712      True  \n","1         0.000000e+00       0.000000     False  \n","2         4.510187e-13       0.016118      True  \n","3         0.000000e+00       0.000000     False  \n","4         3.032936e-13       0.022296      True  \n","5         0.000000e+00       0.000000     False  \n","6         1.371213e-13       0.026575      True  \n","7         0.000000e+00       0.000000     False  \n","8         8.967040e-14       0.042257      True  \n","9         0.000000e+00       0.000000     False  \n","10        3.946946e-14       0.052955      True  \n","11        0.000000e+00       0.000000     False  \n","12        4.068194e-14       0.056503      True  \n","13        0.000000e+00       0.000000     False  \n","14        3.210387e-14       0.100775      True  \n","15        0.000000e+00       0.000000     False  \n","16        1.667407e-14       0.133778      True  \n","17        0.000000e+00       0.000000     False  \n","18        1.527773e-14       0.133899      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.087360e-14       0.149696      True  \n","21        0.000000e+00       0.000000     False  \n","22        2.199752e-14       0.122990      True  \n","23        0.000000e+00       0.000000     False  \n","24        2.426836e-14       0.125983      True  \n","25        0.000000e+00       0.000000     False  \n","26        1.646431e-11       0.012463      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 1475792/14719818 (0.1003)\n","FLOP Sparsity: 63873673/313478154 (0.2038)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner grasp --compression 1 --post-epochs 10 --expid cifar_vgg_grasp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AASLuAmp8-n_","executionInfo":{"status":"ok","timestamp":1677454554467,"user_tz":300,"elapsed":370008,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"8b78b8f9-bfb0-475f-8337-c5574353cc32"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment 'singleshot' with expid 'cifar_vgg_grasp' exists.  Overwrite (yes/no)? yes\n","Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.615866244000017\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:01<00:00,  1.61s/it]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:23<00:00, 26.36s/it]\n","Time_post_train:  265.07665641099993\n","Train results:\n","                 train_loss     test_loss  top1_accuracy  top5_accuracy\n","Init.      0           NaN  2.417717e+00          11.73          50.17\n","Pre-Prune  0           NaN  2.417717e+00          11.73          50.17\n","Post-Prune 0           NaN  7.798163e+09          10.00          50.00\n","Final      10  5169.130754  6.594397e+03          11.18          52.30\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.538194     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.370334    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.307170    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.223050   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.190138   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.143294   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.146985   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.124977  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.081373  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.085475  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.087296  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.085676  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.094220  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.526758     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   4.450945e-05    8.900291e-08   0.076912    1.538199e-04   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   2.075134e-06    4.294618e-09   0.076498    2.520945e-05   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   1.038030e-06    1.728651e-09   0.076532    1.590854e-05   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   5.169493e-07    5.200354e-10   0.076227    7.789511e-06   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   2.561208e-07    2.032912e-10   0.075533    4.886558e-06   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  1.238564e-07    5.129472e-11   0.073054    2.461394e-06   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  1.238643e-07    3.226695e-11   0.073058    2.103923e-06   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  6.004534e-08    1.234029e-11   0.070832    1.425750e-06   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  2.802450e-08    3.534325e-12   0.066118    7.976792e-07   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  2.819010e-08    2.922252e-12   0.066509    7.932248e-07   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  2.725771e-08    3.176401e-12   0.064309    7.930591e-07   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  2.784332e-08    3.840922e-12   0.065691    7.952294e-07   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  2.729167e-08    4.988742e-12   0.064389    9.026103e-07   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  1.451916e-05    4.613413e-09   0.074338    3.635621e-05   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         6.732343e-08       0.265801      True  \n","1         0.000000e+00       0.000000     False  \n","2         3.663408e-09       0.929321      True  \n","3         0.000000e+00       0.000000     False  \n","4         1.476646e-09       1.172905      True  \n","5         0.000000e+00       0.000000     False  \n","6         4.596262e-10       1.148610      True  \n","7         0.000000e+00       0.000000     False  \n","8         1.794783e-10       1.441105      True  \n","9         0.000000e+00       0.000000     False  \n","10        4.525161e-11       1.451789      True  \n","11        0.000000e+00       0.000000     False  \n","12        2.785579e-11       1.240944      True  \n","13        0.000000e+00       0.000000     False  \n","14        1.031113e-11       1.681883      True  \n","15        0.000000e+00       0.000000     False  \n","16        2.898821e-12       1.881961      True  \n","17        0.000000e+00       0.000000     False  \n","18        2.293842e-12       1.871452      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.548202e-12       1.871061      True  \n","21        0.000000e+00       0.000000     False  \n","22        3.209310e-12       1.876181      True  \n","23        0.000000e+00       0.000000     False  \n","24        4.174782e-12       2.129525      True  \n","25        0.000000e+00       0.000000     False  \n","26        3.502445e-09       0.186144      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 1475793/14719818 (0.1003)\n","FLOP Sparsity: 55153682/313478154 (0.1759)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --compression 1 --post-epochs 10 --expid cifar_vgg_synflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-ggTrcS9ATU","executionInfo":{"status":"ok","timestamp":1677455352935,"user_tz":300,"elapsed":285340,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"3eba1b4a-2b3f-4800-e121-28a9e44a0d1b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment 'singleshot' with expid 'cifar_vgg_synflow' exists.  Overwrite (yes/no)? yes\n","Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.6706950620000498\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.88it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:27<00:00, 26.70s/it]\n","Time_post_train:  268.5496590390003\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302601          10.00          49.94\n","Final      10    0.500457   0.551291          81.39          99.01\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.997106     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.964790    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.928589    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.859741   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.726661   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.484929   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.484746   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.162064  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.007960  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.007958  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.029585  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.029672  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.033553  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.994922     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance     score sum  score abs mean  \\\n","0   1.704298e+19             inf  2.945027e+22    1.704298e+19   \n","1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","2   7.988898e+17             inf  2.945027e+22    7.988898e+17   \n","3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","4   3.994448e+17             inf  2.945027e+22    3.994448e+17   \n","5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","6   1.997224e+17             inf  2.945027e+22    1.997224e+17   \n","7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","8   9.986122e+16             inf  2.945027e+22    9.986122e+16   \n","9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","10  4.993060e+16             inf  2.945027e+22    4.993060e+16   \n","11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","12  4.993061e+16             inf  2.945027e+22    4.993061e+16   \n","13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","14  2.496530e+16             inf  2.945027e+22    2.496530e+16   \n","15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   \n","17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   \n","19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","20  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","22  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","24  1.248265e+16             inf  2.945028e+22    1.248265e+16   \n","25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","26  5.752006e+18             inf  2.945027e+22    5.752006e+18   \n","27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0                  inf   2.945027e+22      True  \n","1         0.000000e+00   0.000000e+00     False  \n","2                  inf   2.945027e+22      True  \n","3         0.000000e+00   0.000000e+00     False  \n","4                  inf   2.945027e+22      True  \n","5         0.000000e+00   0.000000e+00     False  \n","6                  inf   2.945027e+22      True  \n","7         0.000000e+00   0.000000e+00     False  \n","8                  inf   2.945027e+22      True  \n","9         0.000000e+00   0.000000e+00     False  \n","10                 inf   2.945027e+22      True  \n","11        0.000000e+00   0.000000e+00     False  \n","12                 inf   2.945027e+22      True  \n","13        0.000000e+00   0.000000e+00     False  \n","14                 inf   2.945027e+22      True  \n","15        0.000000e+00   0.000000e+00     False  \n","16        9.402646e+31   2.945026e+22      True  \n","17        0.000000e+00   0.000000e+00     False  \n","18        9.415211e+31   2.945027e+22      True  \n","19        0.000000e+00   0.000000e+00     False  \n","20                 inf   2.945026e+22      True  \n","21        0.000000e+00   0.000000e+00     False  \n","22                 inf   2.945026e+22      True  \n","23        0.000000e+00   0.000000e+00     False  \n","24                 inf   2.945028e+22      True  \n","25        0.000000e+00   0.000000e+00     False  \n","26                 inf   2.945027e+22      True  \n","27        0.000000e+00   0.000000e+00     False  \n","Parameter Sparsity: 1475793/14719818 (0.1003)\n","FLOP Sparsity: 143301085/313478154 (0.4571)\n","Saving results.\n"]}]},{"cell_type":"markdown","source":["mnist_fc"],"metadata":{"id":"3dn96y0VWoqb"}},{"cell_type":"code","source":["!python main.py --model-class default --model fc --dataset mnist --experiment singleshot --pruner rand --compression 1 --post-epochs 10 --expid mnist_fc_rand"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tSQjqiuD9TE3","executionInfo":{"status":"ok","timestamp":1677455392194,"user_tz":300,"elapsed":39264,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"50f91e03-67f5-497e-d009-459480b76a27"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading mnist dataset.\n","Creating default-fc model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  0.5466974519999894\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00, 345.15it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [00:31<00:00,  3.11s/it]\n","Time_post_train:  31.646524885999952\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.306855          10.32          47.39\n","Pre-Prune  0          NaN   2.306855          10.32          47.39\n","Post-Prune 0          NaN   2.305038          10.32          49.26\n","Final      10    0.189619   0.195772          94.00          99.77\n","Prune results:\n","    module   param  sparsity   size       shape  flops  score mean  \\\n","0       1  weight  0.100969  78400  (100, 784)  78400    0.001971   \n","1       1    bias  1.000000    100      (100,)    100    0.000000   \n","2       3  weight  0.098400  10000  (100, 100)  10000   -0.004195   \n","3       3    bias  1.000000    100      (100,)    100    0.000000   \n","4       5  weight  0.099500  10000  (100, 100)  10000    0.010099   \n","5       5    bias  1.000000    100      (100,)    100    0.000000   \n","6       7  weight  0.097400  10000  (100, 100)  10000   -0.002534   \n","7       7    bias  1.000000    100      (100,)    100    0.000000   \n","8       9  weight  0.096800  10000  (100, 100)  10000   -0.012221   \n","9       9    bias  1.000000    100      (100,)    100    0.000000   \n","10     11  weight  0.103000   1000   (10, 100)   1000   -0.024213   \n","11     11    bias  1.000000     10       (10,)     10    0.000000   \n","\n","    score variance   score sum  score abs mean  score abs variance  \\\n","0         1.007007  154.535782        0.800812            0.365711   \n","1         0.000000    0.000000        0.000000            0.000000   \n","2         0.995103  -41.945179        0.792052            0.367775   \n","3         0.000000    0.000000        0.000000            0.000000   \n","4         0.975469  100.989647        0.789619            0.352073   \n","5         0.000000    0.000000        0.000000            0.000000   \n","6         0.988294  -25.344933        0.791694            0.361521   \n","7         0.000000    0.000000        0.000000            0.000000   \n","8         0.982350 -122.213615        0.792565            0.354341   \n","9         0.000000    0.000000        0.000000            0.000000   \n","10        1.035793  -24.212658        0.826184            0.353799   \n","11        0.000000    0.000000        0.000000            0.000000   \n","\n","    score abs sum  prunable  \n","0    62783.628906      True  \n","1        0.000000     False  \n","2     7920.516602      True  \n","3        0.000000     False  \n","4     7896.190430      True  \n","5        0.000000     False  \n","6     7916.939453      True  \n","7        0.000000     False  \n","8     7925.648438      True  \n","9        0.000000     False  \n","10     826.184204      True  \n","11       0.000000     False  \n","Parameter Sparsity: 12450/119910 (0.1038)\n","FLOP Sparsity: 12450/119910 (0.1038)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class default --model fc --dataset mnist --experiment singleshot --pruner mag --compression 1 --post-epochs 10 --pre-epochs 10 --expid mnist_fc_mag"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fruEgNTqWrc2","executionInfo":{"status":"ok","timestamp":1677455461982,"user_tz":300,"elapsed":69799,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"aed3538c-1236-4272-98fb-fdad154321af"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading mnist dataset.\n","Creating default-fc model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [00:30<00:00,  3.09s/it]\n","Time_pre_train:  31.49518290700007\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00, 338.00it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [00:32<00:00,  3.29s/it]\n","Time_post_train:  33.40290963100006\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.306855          10.32          47.39\n","Pre-Prune  10    0.033572   0.095857          97.23          99.94\n","Post-Prune 0          NaN   1.855864          64.82          94.50\n","Final      10    0.027902   0.085134          97.58          99.95\n","Prune results:\n","    module   param  sparsity   size       shape  flops  score mean  \\\n","0       1  weight   0.05037  78400  (100, 784)  78400    0.035574   \n","1       1    bias   1.00000    100      (100,)    100    0.000000   \n","2       3  weight   0.20690  10000  (100, 100)  10000    0.067603   \n","3       3    bias   1.00000    100      (100,)    100    0.000000   \n","4       5  weight   0.20290  10000  (100, 100)  10000    0.066043   \n","5       5    bias   1.00000    100      (100,)    100    0.000000   \n","6       7  weight   0.19220  10000  (100, 100)  10000    0.064650   \n","7       7    bias   1.00000    100      (100,)    100    0.000000   \n","8       9  weight   0.16890  10000  (100, 100)  10000    0.061396   \n","9       9    bias   1.00000    100      (100,)    100    0.000000   \n","10     11  weight   0.28200   1000   (10, 100)   1000    0.072717   \n","11     11    bias   1.00000     10       (10,)     10    0.000000   \n","\n","    score variance    score sum  score abs mean  score abs variance  \\\n","0         0.001099  2788.962402        0.035574            0.001099   \n","1         0.000000     0.000000        0.000000            0.000000   \n","2         0.002385   676.031311        0.067603            0.002385   \n","3         0.000000     0.000000        0.000000            0.000000   \n","4         0.002176   660.431152        0.066043            0.002176   \n","5         0.000000     0.000000        0.000000            0.000000   \n","6         0.001994   646.497070        0.064650            0.001994   \n","7         0.000000     0.000000        0.000000            0.000000   \n","8         0.001750   613.958740        0.061396            0.001750   \n","9         0.000000     0.000000        0.000000            0.000000   \n","10        0.002277    72.717285        0.072717            0.002277   \n","11        0.000000     0.000000        0.000000            0.000000   \n","\n","    score abs sum  prunable  \n","0     2788.962402      True  \n","1        0.000000     False  \n","2      676.031311      True  \n","3        0.000000     False  \n","4      660.431152      True  \n","5        0.000000     False  \n","6      646.497070      True  \n","7        0.000000     False  \n","8      613.958740      True  \n","9        0.000000     False  \n","10      72.717285      True  \n","11       0.000000     False  \n","Parameter Sparsity: 12450/119910 (0.1038)\n","FLOP Sparsity: 12450/119910 (0.1038)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class default --model fc --dataset mnist --experiment singleshot --pruner snip --compression 1 --post-epochs 10 --expid mnist_fc_snip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-SBzxg0XBw0","executionInfo":{"status":"ok","timestamp":1677455500802,"user_tz":300,"elapsed":38838,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"78d92a61-6659-4635-bb80-56fcd0514365"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading mnist dataset.\n","Creating default-fc model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  0.6146875330000512\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  5.31it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [00:32<00:00,  3.28s/it]\n","Time_post_train:  33.433621559999665\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.306855          10.32          47.39\n","Pre-Prune  0          NaN   2.306855          10.32          47.39\n","Post-Prune 0          NaN   2.306854          10.32          49.80\n","Final      10    0.130075   0.137554          95.56          99.81\n","Prune results:\n","    module   param  sparsity   size       shape  flops  score mean  \\\n","0       1  weight  0.054056  78400  (100, 784)  78400    0.000005   \n","1       1    bias  1.000000    100      (100,)    100    0.000000   \n","2       3  weight  0.218900  10000  (100, 100)  10000    0.000013   \n","3       3    bias  1.000000    100      (100,)    100    0.000000   \n","4       5  weight  0.181800  10000  (100, 100)  10000    0.000011   \n","5       5    bias  1.000000    100      (100,)    100    0.000000   \n","6       7  weight  0.151100  10000  (100, 100)  10000    0.000010   \n","7       7    bias  1.000000    100      (100,)    100    0.000000   \n","8       9  weight  0.176600  10000  (100, 100)  10000    0.000016   \n","9       9    bias  1.000000    100      (100,)    100    0.000000   \n","10     11  weight  0.418000   1000   (10, 100)   1000    0.000096   \n","11     11    bias  1.000000     10       (10,)     10    0.000000   \n","\n","    score variance  score sum  score abs mean  score abs variance  \\\n","0     5.989315e-11   0.404448        0.000005        5.989315e-11   \n","1     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","2     4.284687e-10   0.125934        0.000013        4.284687e-10   \n","3     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","4     4.830765e-10   0.111812        0.000011        4.830765e-10   \n","5     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","6     6.846701e-10   0.103630        0.000010        6.846701e-10   \n","7     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","8     2.011727e-09   0.158670        0.000016        2.011727e-09   \n","9     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","10    5.414625e-08   0.095505        0.000096        5.414625e-08   \n","11    0.000000e+00   0.000000        0.000000        0.000000e+00   \n","\n","    score abs sum  prunable  \n","0        0.404448      True  \n","1        0.000000     False  \n","2        0.125934      True  \n","3        0.000000     False  \n","4        0.111812      True  \n","5        0.000000     False  \n","6        0.103630      True  \n","7        0.000000     False  \n","8        0.158670      True  \n","9        0.000000     False  \n","10       0.095505      True  \n","11       0.000000     False  \n","Parameter Sparsity: 12449/119910 (0.1038)\n","FLOP Sparsity: 12449/119910 (0.1038)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class default --model fc --dataset mnist --experiment singleshot --pruner grasp --compression 1 --post-epochs 10 --expid mnist_fc_grasp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tpW8XoyXLVe","executionInfo":{"status":"ok","timestamp":1677455538302,"user_tz":300,"elapsed":37514,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"e4a9ecb4-3d4a-4331-b090-b82ee4440373"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading mnist dataset.\n","Creating default-fc model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  0.5651695789997575\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.67it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [00:31<00:00,  3.15s/it]\n","Time_post_train:  32.03964713200003\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.306855          10.32          47.39\n","Pre-Prune  0          NaN   2.306855          10.32          47.39\n","Post-Prune 0          NaN   2.387675          10.08          49.31\n","Final      10    0.163665   0.166841          95.09          99.75\n","Prune results:\n","    module   param  sparsity   size       shape  flops    score mean  \\\n","0       1  weight  0.051645  78400  (100, 784)  78400  8.022544e-07   \n","1       1    bias  1.000000    100      (100,)    100  0.000000e+00   \n","2       3  weight  0.216500  10000  (100, 100)  10000  1.698872e-05   \n","3       3    bias  1.000000    100      (100,)    100  0.000000e+00   \n","4       5  weight  0.206200  10000  (100, 100)  10000  1.701438e-05   \n","5       5    bias  1.000000    100      (100,)    100  0.000000e+00   \n","6       7  weight  0.176400  10000  (100, 100)  10000  1.790806e-05   \n","7       7    bias  1.000000    100      (100,)    100  0.000000e+00   \n","8       9  weight  0.156500  10000  (100, 100)  10000  2.057313e-05   \n","9       9    bias  1.000000    100      (100,)    100  0.000000e+00   \n","10     11  weight  0.335000   1000   (10, 100)   1000  2.119170e-04   \n","11     11    bias  1.000000     10       (10,)     10  0.000000e+00   \n","\n","    score variance  score sum  score abs mean  score abs variance  \\\n","0     5.569648e-10   0.062897        0.000013        4.010847e-10   \n","1     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","2     9.410978e-09   0.169887        0.000045        7.668780e-09   \n","3     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","4     1.349678e-08   0.170144        0.000052        1.109022e-08   \n","5     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","6     2.519507e-08   0.179081        0.000061        2.182158e-08   \n","7     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","8     3.435935e-08   0.205731        0.000066        3.037865e-08   \n","9     0.000000e+00   0.000000        0.000000        0.000000e+00   \n","10    4.173014e-07   0.211917        0.000315        3.629367e-07   \n","11    0.000000e+00   0.000000        0.000000        0.000000e+00   \n","\n","    score abs sum  prunable  \n","0        0.980858      True  \n","1        0.000000     False  \n","2        0.450646      True  \n","3        0.000000     False  \n","4        0.519234      True  \n","5        0.000000     False  \n","6        0.607798      True  \n","7        0.000000     False  \n","8        0.663623      True  \n","9        0.000000     False  \n","10       0.315077      True  \n","11       0.000000     False  \n","Parameter Sparsity: 12450/119910 (0.1038)\n","FLOP Sparsity: 12450/119910 (0.1038)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class default --model fc --dataset mnist --experiment singleshot --pruner synflow --compression 1 --post-epochs 10 --expid mnist_fc_synflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ne6iCi5oXdhP","executionInfo":{"status":"ok","timestamp":1677455575620,"user_tz":300,"elapsed":37335,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"92ec8df9-4eaa-4f5c-ad7d-54c89fd543e2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading mnist dataset.\n","Creating default-fc model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  0.5852375800000118\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  5.11it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [00:31<00:00,  3.17s/it]\n","Time_post_train:  32.238599498999974\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.306855          10.32          47.39\n","Pre-Prune  0          NaN   2.306855          10.32          47.39\n","Post-Prune 0          NaN   2.306644          10.32          49.80\n","Final      10    2.301366   2.301024          11.35          52.14\n","Prune results:\n","    module   param  sparsity   size       shape  flops  score mean  \\\n","0       1  weight    0.0000  78400  (100, 784)  78400    5.375414   \n","1       1    bias    1.0000    100      (100,)    100    0.000000   \n","2       3  weight    0.2801  10000  (100, 100)  10000   42.197376   \n","3       3    bias    1.0000    100      (100,)    100    0.000000   \n","4       5  weight    0.2774  10000  (100, 100)  10000   42.228401   \n","5       5    bias    1.0000    100      (100,)    100    0.000000   \n","6       7  weight    0.2796  10000  (100, 100)  10000   42.234432   \n","7       7    bias    1.0000    100      (100,)    100    0.000000   \n","8       9  weight    0.2638  10000  (100, 100)  10000   42.235725   \n","9       9    bias    1.0000    100      (100,)    100    0.000000   \n","10     11  weight    0.9310   1000   (10, 100)   1000  422.359283   \n","11     11    bias    1.0000     10       (10,)     10    0.000000   \n","\n","    score variance     score sum  score abs mean  score abs variance  \\\n","0         9.872970  421432.43750        5.375414            9.872970   \n","1         0.000000       0.00000        0.000000            0.000000   \n","2       606.436279  421973.75000       42.197376          606.436279   \n","3         0.000000       0.00000        0.000000            0.000000   \n","4       605.871582  422284.00000       42.228401          605.871582   \n","5         0.000000       0.00000        0.000000            0.000000   \n","6       616.338806  422344.31250       42.234432          616.338806   \n","7         0.000000       0.00000        0.000000            0.000000   \n","8       690.546692  422357.25000       42.235725          690.546692   \n","9         0.000000       0.00000        0.000000            0.000000   \n","10    62346.089844  422359.28125      422.359283        62346.089844   \n","11        0.000000       0.00000        0.000000            0.000000   \n","\n","    score abs sum  prunable  \n","0    421432.43750      True  \n","1         0.00000     False  \n","2    421973.75000      True  \n","3         0.00000     False  \n","4    422284.00000      True  \n","5         0.00000     False  \n","6    422344.31250      True  \n","7         0.00000     False  \n","8    422357.25000      True  \n","9         0.00000     False  \n","10   422359.28125      True  \n","11        0.00000     False  \n","Parameter Sparsity: 12449/119910 (0.1038)\n","FLOP Sparsity: 12449/119910 (0.1038)\n","Saving results.\n"]}]},{"cell_type":"markdown","source":["compression ratio"],"metadata":{"id":"PFX8zFBDdUCM"}},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner rand  --post-epochs 10 --compression 0.05 --expid rand_0_05"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WR0ViKrwdjQf","executionInfo":{"status":"ok","timestamp":1677444559368,"user_tz":300,"elapsed":285112,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"48d1c406-9fb4-4fb1-ac1b-63999b8a6b53"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment 'singleshot' with expid 'rand_0_05' exists.  Overwrite (yes/no)? yes\n","Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.83it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:26<00:00, 26.65s/it]\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.344280          10.90          49.75\n","Final      10    0.629289   0.669232          78.25          97.66\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.881944     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.892659    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.892320    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.891249   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.890605   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.891595   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.891544   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.891028  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.891056  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.891362  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.891020  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.891372  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.891427  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.892187     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance    score sum  score abs mean  \\\n","0    -0.027050        1.005513   -46.742134        0.805873   \n","1     0.000000        0.000000     0.000000        0.000000   \n","2     0.000872        0.996963    32.139240        0.796522   \n","3     0.000000        0.000000     0.000000        0.000000   \n","4     0.001659        1.001431   122.280731        0.798236   \n","5     0.000000        0.000000     0.000000        0.000000   \n","6    -0.002633        0.996887  -388.247467        0.796746   \n","7     0.000000        0.000000     0.000000        0.000000   \n","8    -0.001425        1.002429  -420.358032        0.799537   \n","9     0.000000        0.000000     0.000000        0.000000   \n","10    0.001097        1.001136   647.131470        0.798347   \n","11    0.000000        0.000000     0.000000        0.000000   \n","12    0.000697        0.998635   411.355530        0.797115   \n","13    0.000000        0.000000     0.000000        0.000000   \n","14    0.000298        1.001965   351.712585        0.798915   \n","15    0.000000        0.000000     0.000000        0.000000   \n","16    0.000297        1.000542   700.673950        0.797925   \n","17    0.000000        0.000000     0.000000        0.000000   \n","18    0.000261        0.999856   616.478821        0.797893   \n","19    0.000000        0.000000     0.000000        0.000000   \n","20   -0.000339        0.999540  -800.459961        0.797691   \n","21    0.000000        0.000000     0.000000        0.000000   \n","22   -0.000862        0.997625 -2034.541504        0.796859   \n","23    0.000000        0.000000     0.000000        0.000000   \n","24    0.000637        0.999602  1502.357544        0.797581   \n","25    0.000000        0.000000     0.000000        0.000000   \n","26    0.006237        1.002876    31.934721        0.798595   \n","27    0.000000        0.000000     0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.356814   1.392548e+03      True  \n","1             0.000000   0.000000e+00     False  \n","2             0.362516   2.936300e+04      True  \n","3             0.000000   0.000000e+00     False  \n","4             0.364253   5.885236e+04      True  \n","5             0.000000   0.000000e+00     False  \n","6             0.362089   1.174851e+05      True  \n","7             0.000000   0.000000e+00     False  \n","8             0.363171   2.357931e+05      True  \n","9             0.000000   0.000000e+00     False  \n","10            0.363779   4.708842e+05      True  \n","11            0.000000   0.000000e+00     False  \n","12            0.363243   4.701573e+05      True  \n","13            0.000000   0.000000e+00     False  \n","14            0.363700   9.424383e+05      True  \n","15            0.000000   0.000000e+00     False  \n","16            0.363858   1.882542e+06      True  \n","17            0.000000   0.000000e+00     False  \n","18            0.363222   1.882466e+06      True  \n","19            0.000000   0.000000e+00     False  \n","20            0.363228   1.881990e+06      True  \n","21            0.000000   0.000000e+00     False  \n","22            0.362642   1.880025e+06      True  \n","23            0.000000   0.000000e+00     False  \n","24            0.363466   1.881731e+06      True  \n","25            0.000000   0.000000e+00     False  \n","26            0.365161   4.088806e+03      True  \n","27            0.000000   0.000000e+00     False  \n","Parameter Sparsity: 13119513/14719818 (0.8913)\n","FLOP Sparsity: 279479681/313478154 (0.8915)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner rand  --post-epochs 10 --compression 0.1 --expid rand_0_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCm1UOIWd_ni","executionInfo":{"status":"ok","timestamp":1677444858623,"user_tz":300,"elapsed":297630,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"edc9620b-4ed9-4fa4-e3af-a31656e01aeb"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.42it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:45<00:00, 28.52s/it]\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.308359          10.00          50.59\n","Final      10    0.580793   0.636985          79.00          98.31\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.780671     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.795030    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.794908    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.793301   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.792999   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.794490   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.795032   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.793772  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.794418  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.794145  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.794286  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.794382  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.794690  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.791602     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance    score sum  score abs mean  \\\n","0    -0.027050        1.005513   -46.742134        0.805873   \n","1     0.000000        0.000000     0.000000        0.000000   \n","2     0.000872        0.996963    32.139240        0.796522   \n","3     0.000000        0.000000     0.000000        0.000000   \n","4     0.001659        1.001431   122.280731        0.798236   \n","5     0.000000        0.000000     0.000000        0.000000   \n","6    -0.002633        0.996887  -388.247467        0.796746   \n","7     0.000000        0.000000     0.000000        0.000000   \n","8    -0.001425        1.002429  -420.358032        0.799537   \n","9     0.000000        0.000000     0.000000        0.000000   \n","10    0.001097        1.001136   647.131470        0.798347   \n","11    0.000000        0.000000     0.000000        0.000000   \n","12    0.000697        0.998635   411.355530        0.797115   \n","13    0.000000        0.000000     0.000000        0.000000   \n","14    0.000298        1.001965   351.712585        0.798915   \n","15    0.000000        0.000000     0.000000        0.000000   \n","16    0.000297        1.000542   700.673950        0.797925   \n","17    0.000000        0.000000     0.000000        0.000000   \n","18    0.000261        0.999856   616.478821        0.797893   \n","19    0.000000        0.000000     0.000000        0.000000   \n","20   -0.000339        0.999540  -800.459961        0.797691   \n","21    0.000000        0.000000     0.000000        0.000000   \n","22   -0.000862        0.997625 -2034.541504        0.796859   \n","23    0.000000        0.000000     0.000000        0.000000   \n","24    0.000637        0.999602  1502.357544        0.797581   \n","25    0.000000        0.000000     0.000000        0.000000   \n","26    0.006237        1.002876    31.934721        0.798595   \n","27    0.000000        0.000000     0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.356814   1.392548e+03      True  \n","1             0.000000   0.000000e+00     False  \n","2             0.362516   2.936300e+04      True  \n","3             0.000000   0.000000e+00     False  \n","4             0.364253   5.885236e+04      True  \n","5             0.000000   0.000000e+00     False  \n","6             0.362089   1.174851e+05      True  \n","7             0.000000   0.000000e+00     False  \n","8             0.363171   2.357931e+05      True  \n","9             0.000000   0.000000e+00     False  \n","10            0.363779   4.708842e+05      True  \n","11            0.000000   0.000000e+00     False  \n","12            0.363243   4.701573e+05      True  \n","13            0.000000   0.000000e+00     False  \n","14            0.363700   9.424383e+05      True  \n","15            0.000000   0.000000e+00     False  \n","16            0.363858   1.882542e+06      True  \n","17            0.000000   0.000000e+00     False  \n","18            0.363222   1.882466e+06      True  \n","19            0.000000   0.000000e+00     False  \n","20            0.363228   1.881990e+06      True  \n","21            0.000000   0.000000e+00     False  \n","22            0.362642   1.880025e+06      True  \n","23            0.000000   0.000000e+00     False  \n","24            0.363466   1.881731e+06      True  \n","25            0.000000   0.000000e+00     False  \n","26            0.365161   4.088806e+03      True  \n","27            0.000000   0.000000e+00     False  \n","Parameter Sparsity: 11693237/14719818 (0.7944)\n","FLOP Sparsity: 249032952/313478154 (0.7944)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner rand  --post-epochs 10 --compression 0.2 --expid rand_0_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"izgKW5OCeSm3","executionInfo":{"status":"ok","timestamp":1677445150731,"user_tz":300,"elapsed":292111,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"bb523c40-f017-4a95-8e4d-be156075b188"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.41it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:39<00:00, 27.95s/it]\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302913          10.00          52.25\n","Final      10    0.525361   0.573587          81.36          98.56\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.615162     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.631673    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.630900    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.629584   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.629442   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.631339   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.632000   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.630888  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.631264  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.630828  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.630807  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.630661  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.631167  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.638281     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance    score sum  score abs mean  \\\n","0    -0.027050        1.005513   -46.742134        0.805873   \n","1     0.000000        0.000000     0.000000        0.000000   \n","2     0.000872        0.996963    32.139240        0.796522   \n","3     0.000000        0.000000     0.000000        0.000000   \n","4     0.001659        1.001431   122.280731        0.798236   \n","5     0.000000        0.000000     0.000000        0.000000   \n","6    -0.002633        0.996887  -388.247467        0.796746   \n","7     0.000000        0.000000     0.000000        0.000000   \n","8    -0.001425        1.002429  -420.358032        0.799537   \n","9     0.000000        0.000000     0.000000        0.000000   \n","10    0.001097        1.001136   647.131470        0.798347   \n","11    0.000000        0.000000     0.000000        0.000000   \n","12    0.000697        0.998635   411.355530        0.797115   \n","13    0.000000        0.000000     0.000000        0.000000   \n","14    0.000298        1.001965   351.712585        0.798915   \n","15    0.000000        0.000000     0.000000        0.000000   \n","16    0.000297        1.000542   700.673950        0.797925   \n","17    0.000000        0.000000     0.000000        0.000000   \n","18    0.000261        0.999856   616.478821        0.797893   \n","19    0.000000        0.000000     0.000000        0.000000   \n","20   -0.000339        0.999540  -800.459961        0.797691   \n","21    0.000000        0.000000     0.000000        0.000000   \n","22   -0.000862        0.997625 -2034.541504        0.796859   \n","23    0.000000        0.000000     0.000000        0.000000   \n","24    0.000637        0.999602  1502.357544        0.797581   \n","25    0.000000        0.000000     0.000000        0.000000   \n","26    0.006237        1.002876    31.934721        0.798595   \n","27    0.000000        0.000000     0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.356814   1.392548e+03      True  \n","1             0.000000   0.000000e+00     False  \n","2             0.362516   2.936300e+04      True  \n","3             0.000000   0.000000e+00     False  \n","4             0.364253   5.885236e+04      True  \n","5             0.000000   0.000000e+00     False  \n","6             0.362089   1.174851e+05      True  \n","7             0.000000   0.000000e+00     False  \n","8             0.363171   2.357931e+05      True  \n","9             0.000000   0.000000e+00     False  \n","10            0.363779   4.708842e+05      True  \n","11            0.000000   0.000000e+00     False  \n","12            0.363243   4.701573e+05      True  \n","13            0.000000   0.000000e+00     False  \n","14            0.363700   9.424383e+05      True  \n","15            0.000000   0.000000e+00     False  \n","16            0.363858   1.882542e+06      True  \n","17            0.000000   0.000000e+00     False  \n","18            0.363222   1.882466e+06      True  \n","19            0.000000   0.000000e+00     False  \n","20            0.363228   1.881990e+06      True  \n","21            0.000000   0.000000e+00     False  \n","22            0.362642   1.880025e+06      True  \n","23            0.000000   0.000000e+00     False  \n","24            0.363466   1.881731e+06      True  \n","25            0.000000   0.000000e+00     False  \n","26            0.365161   4.088806e+03      True  \n","27            0.000000   0.000000e+00     False  \n","Parameter Sparsity: 9289140/14719818 (0.6311)\n","FLOP Sparsity: 197867910/313478154 (0.6312)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner rand  --post-epochs 10 --compression 0.5 --expid rand_0_5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFF6PhgzeVkZ","executionInfo":{"status":"ok","timestamp":1677445442287,"user_tz":300,"elapsed":291561,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"66b0dc52-341d-411f-981e-543291817e75"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00,  8.31it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:39<00:00, 27.94s/it]\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302586           9.93          49.00\n","Final      10    0.626187   0.633832          78.57          98.47\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.306713     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.314372    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.317084    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.315186   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.317088   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.316661   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.316635   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.316862  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.316330  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.316291  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.316003  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.315838  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.316113  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.318164     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance    score sum  score abs mean  \\\n","0    -0.027050        1.005513   -46.742134        0.805873   \n","1     0.000000        0.000000     0.000000        0.000000   \n","2     0.000872        0.996963    32.139240        0.796522   \n","3     0.000000        0.000000     0.000000        0.000000   \n","4     0.001659        1.001431   122.280731        0.798236   \n","5     0.000000        0.000000     0.000000        0.000000   \n","6    -0.002633        0.996887  -388.247467        0.796746   \n","7     0.000000        0.000000     0.000000        0.000000   \n","8    -0.001425        1.002429  -420.358032        0.799537   \n","9     0.000000        0.000000     0.000000        0.000000   \n","10    0.001097        1.001136   647.131470        0.798347   \n","11    0.000000        0.000000     0.000000        0.000000   \n","12    0.000697        0.998635   411.355530        0.797115   \n","13    0.000000        0.000000     0.000000        0.000000   \n","14    0.000298        1.001965   351.712585        0.798915   \n","15    0.000000        0.000000     0.000000        0.000000   \n","16    0.000297        1.000542   700.673950        0.797925   \n","17    0.000000        0.000000     0.000000        0.000000   \n","18    0.000261        0.999856   616.478821        0.797893   \n","19    0.000000        0.000000     0.000000        0.000000   \n","20   -0.000339        0.999540  -800.459961        0.797691   \n","21    0.000000        0.000000     0.000000        0.000000   \n","22   -0.000862        0.997625 -2034.541504        0.796859   \n","23    0.000000        0.000000     0.000000        0.000000   \n","24    0.000637        0.999602  1502.357544        0.797581   \n","25    0.000000        0.000000     0.000000        0.000000   \n","26    0.006237        1.002876    31.934721        0.798595   \n","27    0.000000        0.000000     0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.356814   1.392548e+03      True  \n","1             0.000000   0.000000e+00     False  \n","2             0.362516   2.936300e+04      True  \n","3             0.000000   0.000000e+00     False  \n","4             0.364253   5.885236e+04      True  \n","5             0.000000   0.000000e+00     False  \n","6             0.362089   1.174851e+05      True  \n","7             0.000000   0.000000e+00     False  \n","8             0.363171   2.357931e+05      True  \n","9             0.000000   0.000000e+00     False  \n","10            0.363779   4.708842e+05      True  \n","11            0.000000   0.000000e+00     False  \n","12            0.363243   4.701573e+05      True  \n","13            0.000000   0.000000e+00     False  \n","14            0.363700   9.424383e+05      True  \n","15            0.000000   0.000000e+00     False  \n","16            0.363858   1.882542e+06      True  \n","17            0.000000   0.000000e+00     False  \n","18            0.363222   1.882466e+06      True  \n","19            0.000000   0.000000e+00     False  \n","20            0.363228   1.881990e+06      True  \n","21            0.000000   0.000000e+00     False  \n","22            0.362642   1.880025e+06      True  \n","23            0.000000   0.000000e+00     False  \n","24            0.363466   1.881731e+06      True  \n","25            0.000000   0.000000e+00     False  \n","26            0.365161   4.088806e+03      True  \n","27            0.000000   0.000000e+00     False  \n","Parameter Sparsity: 4657710/14719818 (0.3164)\n","FLOP Sparsity: 99268758/313478154 (0.3167)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner rand  --post-epochs 10 --compression 1 --expid rand_1_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRu0kF-Kezcy","executionInfo":{"status":"ok","timestamp":1677445716740,"user_tz":300,"elapsed":274458,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"0c179454-7b12-47e7-81b5-83ea782f4d78"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.43it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:22<00:00, 26.20s/it]\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302585          10.47          50.00\n","Final      10    2.302675   2.302590          10.00          50.00\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.100694     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.100423    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.100898    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.099765   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.099565   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.100325   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.099257   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.100311  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.100044  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.100143  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.100048  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.099625  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.100114  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.103906     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance    score sum  score abs mean  \\\n","0    -0.027050        1.005513   -46.742134        0.805873   \n","1     0.000000        0.000000     0.000000        0.000000   \n","2     0.000872        0.996963    32.139240        0.796522   \n","3     0.000000        0.000000     0.000000        0.000000   \n","4     0.001659        1.001431   122.280731        0.798236   \n","5     0.000000        0.000000     0.000000        0.000000   \n","6    -0.002633        0.996887  -388.247467        0.796746   \n","7     0.000000        0.000000     0.000000        0.000000   \n","8    -0.001425        1.002429  -420.358032        0.799537   \n","9     0.000000        0.000000     0.000000        0.000000   \n","10    0.001097        1.001136   647.131470        0.798347   \n","11    0.000000        0.000000     0.000000        0.000000   \n","12    0.000697        0.998635   411.355530        0.797115   \n","13    0.000000        0.000000     0.000000        0.000000   \n","14    0.000298        1.001965   351.712585        0.798915   \n","15    0.000000        0.000000     0.000000        0.000000   \n","16    0.000297        1.000542   700.673950        0.797925   \n","17    0.000000        0.000000     0.000000        0.000000   \n","18    0.000261        0.999856   616.478821        0.797893   \n","19    0.000000        0.000000     0.000000        0.000000   \n","20   -0.000339        0.999540  -800.459961        0.797691   \n","21    0.000000        0.000000     0.000000        0.000000   \n","22   -0.000862        0.997625 -2034.541504        0.796859   \n","23    0.000000        0.000000     0.000000        0.000000   \n","24    0.000637        0.999602  1502.357544        0.797581   \n","25    0.000000        0.000000     0.000000        0.000000   \n","26    0.006237        1.002876    31.934721        0.798595   \n","27    0.000000        0.000000     0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.356814   1.392548e+03      True  \n","1             0.000000   0.000000e+00     False  \n","2             0.362516   2.936300e+04      True  \n","3             0.000000   0.000000e+00     False  \n","4             0.364253   5.885236e+04      True  \n","5             0.000000   0.000000e+00     False  \n","6             0.362089   1.174851e+05      True  \n","7             0.000000   0.000000e+00     False  \n","8             0.363171   2.357931e+05      True  \n","9             0.000000   0.000000e+00     False  \n","10            0.363779   4.708842e+05      True  \n","11            0.000000   0.000000e+00     False  \n","12            0.363243   4.701573e+05      True  \n","13            0.000000   0.000000e+00     False  \n","14            0.363700   9.424383e+05      True  \n","15            0.000000   0.000000e+00     False  \n","16            0.363858   1.882542e+06      True  \n","17            0.000000   0.000000e+00     False  \n","18            0.363222   1.882466e+06      True  \n","19            0.000000   0.000000e+00     False  \n","20            0.363228   1.881990e+06      True  \n","21            0.000000   0.000000e+00     False  \n","22            0.362642   1.880025e+06      True  \n","23            0.000000   0.000000e+00     False  \n","24            0.363466   1.881731e+06      True  \n","25            0.000000   0.000000e+00     False  \n","26            0.365161   4.088806e+03      True  \n","27            0.000000   0.000000e+00     False  \n","Parameter Sparsity: 1475792/14719818 (0.1003)\n","FLOP Sparsity: 31608901/313478154 (0.1008)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner rand  --post-epochs 10 --compression 2 --expid rand_2_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGVQzb8Me2Zc","executionInfo":{"status":"ok","timestamp":1677445986126,"user_tz":300,"elapsed":269389,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"c1c5bba1-4357-4f05-d2c5-a72c13436752"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Pruning with rand for 1 epochs.\n","100% 1/1 [00:00<00:00,  8.06it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:17<00:00, 25.71s/it]\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302585          10.04          52.25\n","Final      10    2.302675   2.302590          10.00          50.00\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.011574     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.009494    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.010376    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.009854   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.010061   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.010001   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.009896   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.009922  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.010019  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.010017  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.010045  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.009920  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.010064  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.008203     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance    score sum  score abs mean  \\\n","0    -0.027050        1.005513   -46.742134        0.805873   \n","1     0.000000        0.000000     0.000000        0.000000   \n","2     0.000872        0.996963    32.139240        0.796522   \n","3     0.000000        0.000000     0.000000        0.000000   \n","4     0.001659        1.001431   122.280731        0.798236   \n","5     0.000000        0.000000     0.000000        0.000000   \n","6    -0.002633        0.996887  -388.247467        0.796746   \n","7     0.000000        0.000000     0.000000        0.000000   \n","8    -0.001425        1.002429  -420.358032        0.799537   \n","9     0.000000        0.000000     0.000000        0.000000   \n","10    0.001097        1.001136   647.131470        0.798347   \n","11    0.000000        0.000000     0.000000        0.000000   \n","12    0.000697        0.998635   411.355530        0.797115   \n","13    0.000000        0.000000     0.000000        0.000000   \n","14    0.000298        1.001965   351.712585        0.798915   \n","15    0.000000        0.000000     0.000000        0.000000   \n","16    0.000297        1.000542   700.673950        0.797925   \n","17    0.000000        0.000000     0.000000        0.000000   \n","18    0.000261        0.999856   616.478821        0.797893   \n","19    0.000000        0.000000     0.000000        0.000000   \n","20   -0.000339        0.999540  -800.459961        0.797691   \n","21    0.000000        0.000000     0.000000        0.000000   \n","22   -0.000862        0.997625 -2034.541504        0.796859   \n","23    0.000000        0.000000     0.000000        0.000000   \n","24    0.000637        0.999602  1502.357544        0.797581   \n","25    0.000000        0.000000     0.000000        0.000000   \n","26    0.006237        1.002876    31.934721        0.798595   \n","27    0.000000        0.000000     0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.356814   1.392548e+03      True  \n","1             0.000000   0.000000e+00     False  \n","2             0.362516   2.936300e+04      True  \n","3             0.000000   0.000000e+00     False  \n","4             0.364253   5.885236e+04      True  \n","5             0.000000   0.000000e+00     False  \n","6             0.362089   1.174851e+05      True  \n","7             0.000000   0.000000e+00     False  \n","8             0.363171   2.357931e+05      True  \n","9             0.000000   0.000000e+00     False  \n","10            0.363779   4.708842e+05      True  \n","11            0.000000   0.000000e+00     False  \n","12            0.363243   4.701573e+05      True  \n","13            0.000000   0.000000e+00     False  \n","14            0.363700   9.424383e+05      True  \n","15            0.000000   0.000000e+00     False  \n","16            0.363858   1.882542e+06      True  \n","17            0.000000   0.000000e+00     False  \n","18            0.363222   1.882466e+06      True  \n","19            0.000000   0.000000e+00     False  \n","20            0.363228   1.881990e+06      True  \n","21            0.000000   0.000000e+00     False  \n","22            0.362642   1.880025e+06      True  \n","23            0.000000   0.000000e+00     False  \n","24            0.363466   1.881731e+06      True  \n","25            0.000000   0.000000e+00     False  \n","26            0.365161   4.088806e+03      True  \n","27            0.000000   0.000000e+00     False  \n","Parameter Sparsity: 151389/14719818 (0.0103)\n","FLOP Sparsity: 3391191/313478154 (0.0108)\n","Saving results.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5Ao7uLpQnG4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mag"],"metadata":{"id":"ZXEXZmMJnHXa"}},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner mag  --pre-epochs 10 --post-epochs 10 --compression 0.05 --expid mag_0_05"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnAcm3kkns9R","executionInfo":{"status":"ok","timestamp":1677447581931,"user_tz":300,"elapsed":555734,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"977fd893-cc74-49ef-ff36-0b68a8c106bd"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [04:25<00:00, 26.58s/it]\n","Time_pre_train:  267.4622932680004\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.60it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:37<00:00, 27.74s/it]\n","Time_post_train:  279.05774359299994\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  10    0.636168   0.625882          78.96          98.43\n","Post-Prune 0          NaN   0.626722          78.98          98.42\n","Final      10    0.385073   0.514814          83.33          98.82\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.991319     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.957520    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.957384    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.943665   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.943315   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.928577   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.924366   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.924165  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.893676  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.882866  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.877850  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.876492  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.878198  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.958594     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance     score sum  score abs mean  \\\n","0     0.215746        0.026066    372.808960        0.215746   \n","1     0.000000        0.000000      0.000000        0.000000   \n","2     0.053546        0.001713   1973.927124        0.053546   \n","3     0.000000        0.000000      0.000000        0.000000   \n","4     0.054325        0.001772   4005.279297        0.054325   \n","5     0.000000        0.000000      0.000000        0.000000   \n","6     0.041933        0.001118   6183.344727        0.041933   \n","7     0.000000        0.000000      0.000000        0.000000   \n","8     0.041539        0.001103  12250.351562        0.041539   \n","9     0.000000        0.000000      0.000000        0.000000   \n","10    0.034288        0.000832  20223.839844        0.034288   \n","11    0.000000        0.000000      0.000000        0.000000   \n","12    0.032159        0.000741  18968.369141        0.032159   \n","13    0.000000        0.000000      0.000000        0.000000   \n","14    0.032218        0.000737  38005.714844        0.032218   \n","15    0.000000        0.000000      0.000000        0.000000   \n","16    0.023702        0.000453  55919.343750        0.023702   \n","17    0.000000        0.000000      0.000000        0.000000   \n","18    0.020526        0.000309  48428.031250        0.020526   \n","19    0.000000        0.000000      0.000000        0.000000   \n","20    0.019337        0.000255  45621.121094        0.019337   \n","21    0.000000        0.000000      0.000000        0.000000   \n","22    0.018923        0.000234  44643.882812        0.018923   \n","23    0.000000        0.000000      0.000000        0.000000   \n","24    0.019276        0.000248  45477.464844        0.019276   \n","25    0.000000        0.000000      0.000000        0.000000   \n","26    0.049628        0.001454    254.092865        0.049628   \n","27    0.000000        0.000000      0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.026066     372.808960      True  \n","1             0.000000       0.000000     False  \n","2             0.001713    1973.927124      True  \n","3             0.000000       0.000000     False  \n","4             0.001772    4005.279297      True  \n","5             0.000000       0.000000     False  \n","6             0.001118    6183.344727      True  \n","7             0.000000       0.000000     False  \n","8             0.001103   12250.351562      True  \n","9             0.000000       0.000000     False  \n","10            0.000832   20223.839844      True  \n","11            0.000000       0.000000     False  \n","12            0.000741   18968.369141      True  \n","13            0.000000       0.000000     False  \n","14            0.000737   38005.714844      True  \n","15            0.000000       0.000000     False  \n","16            0.000453   55919.343750      True  \n","17            0.000000       0.000000     False  \n","18            0.000309   48428.031250      True  \n","19            0.000000       0.000000     False  \n","20            0.000255   45621.121094      True  \n","21            0.000000       0.000000     False  \n","22            0.000234   44643.882812      True  \n","23            0.000000       0.000000     False  \n","24            0.000248   45477.464844      True  \n","25            0.000000       0.000000     False  \n","26            0.001454     254.092865      True  \n","27            0.000000       0.000000     False  \n","Parameter Sparsity: 13119512/14719818 (0.8913)\n","FLOP Sparsity: 288972557/313478154 (0.9218)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner mag  --pre-epochs 10 --post-epochs 10 --compression 0.1 --expid mag_0_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-AMyWURoEx4","executionInfo":{"status":"ok","timestamp":1677448150640,"user_tz":300,"elapsed":568713,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"d72cd678-87c8-4c00-9166-ce081404a1ec"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [04:36<00:00, 27.67s/it]\n","Time_pre_train:  278.45029897399945\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.30it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:39<00:00, 27.94s/it]\n","Time_post_train:  281.0715045799998\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  10    0.636168   0.625882          78.96          98.43\n","Post-Prune 0          NaN   0.625827          78.97          98.45\n","Final      10    0.384472   0.530674          83.41          98.81\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.980903     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.917453    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.917426    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.892626   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.891876   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.864151   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.855309   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.856236  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.798732  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.778266  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.769715  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.766809  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.769949  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.919141     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance     score sum  score abs mean  \\\n","0     0.215746        0.026066    372.808960        0.215746   \n","1     0.000000        0.000000      0.000000        0.000000   \n","2     0.053546        0.001713   1973.927124        0.053546   \n","3     0.000000        0.000000      0.000000        0.000000   \n","4     0.054325        0.001772   4005.279297        0.054325   \n","5     0.000000        0.000000      0.000000        0.000000   \n","6     0.041933        0.001118   6183.344727        0.041933   \n","7     0.000000        0.000000      0.000000        0.000000   \n","8     0.041539        0.001103  12250.351562        0.041539   \n","9     0.000000        0.000000      0.000000        0.000000   \n","10    0.034288        0.000832  20223.839844        0.034288   \n","11    0.000000        0.000000      0.000000        0.000000   \n","12    0.032159        0.000741  18968.369141        0.032159   \n","13    0.000000        0.000000      0.000000        0.000000   \n","14    0.032218        0.000737  38005.714844        0.032218   \n","15    0.000000        0.000000      0.000000        0.000000   \n","16    0.023702        0.000453  55919.343750        0.023702   \n","17    0.000000        0.000000      0.000000        0.000000   \n","18    0.020526        0.000309  48428.031250        0.020526   \n","19    0.000000        0.000000      0.000000        0.000000   \n","20    0.019337        0.000255  45621.121094        0.019337   \n","21    0.000000        0.000000      0.000000        0.000000   \n","22    0.018923        0.000234  44643.882812        0.018923   \n","23    0.000000        0.000000      0.000000        0.000000   \n","24    0.019276        0.000248  45477.464844        0.019276   \n","25    0.000000        0.000000      0.000000        0.000000   \n","26    0.049628        0.001454    254.092865        0.049628   \n","27    0.000000        0.000000      0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.026066     372.808960      True  \n","1             0.000000       0.000000     False  \n","2             0.001713    1973.927124      True  \n","3             0.000000       0.000000     False  \n","4             0.001772    4005.279297      True  \n","5             0.000000       0.000000     False  \n","6             0.001118    6183.344727      True  \n","7             0.000000       0.000000     False  \n","8             0.001103   12250.351562      True  \n","9             0.000000       0.000000     False  \n","10            0.000832   20223.839844      True  \n","11            0.000000       0.000000     False  \n","12            0.000741   18968.369141      True  \n","13            0.000000       0.000000     False  \n","14            0.000737   38005.714844      True  \n","15            0.000000       0.000000     False  \n","16            0.000453   55919.343750      True  \n","17            0.000000       0.000000     False  \n","18            0.000309   48428.031250      True  \n","19            0.000000       0.000000     False  \n","20            0.000255   45621.121094      True  \n","21            0.000000       0.000000     False  \n","22            0.000234   44643.882812      True  \n","23            0.000000       0.000000     False  \n","24            0.000248   45477.464844      True  \n","25            0.000000       0.000000     False  \n","26            0.001454     254.092865      True  \n","27            0.000000       0.000000     False  \n","Parameter Sparsity: 11693238/14719818 (0.7944)\n","FLOP Sparsity: 266859179/313478154 (0.8513)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner mag  --pre-epochs 10 --post-epochs 10 --compression 0.2 --expid mag_0_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBp7Zpg3oLHg","executionInfo":{"status":"ok","timestamp":1677448718803,"user_tz":300,"elapsed":568166,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"e668734b-0773-4a80-83bc-12cd4aebcbaf"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [04:36<00:00, 27.67s/it]\n","Time_pre_train:  278.5347418700003\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00,  6.98it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:38<00:00, 27.89s/it]\n","Time_post_train:  280.56590833799964\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  10    0.636168   0.625882          78.96          98.43\n","Post-Prune 0          NaN   0.625322          78.84          98.36\n","Final      10    0.376828   0.494488          84.45          99.01\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.965278     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.846544    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.848321    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.802043   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.801602   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.751056   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.736106   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.737184  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.638392  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.602856  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.588731  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.583705  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.588784  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.845508     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance     score sum  score abs mean  \\\n","0     0.215746        0.026066    372.808960        0.215746   \n","1     0.000000        0.000000      0.000000        0.000000   \n","2     0.053546        0.001713   1973.927124        0.053546   \n","3     0.000000        0.000000      0.000000        0.000000   \n","4     0.054325        0.001772   4005.279297        0.054325   \n","5     0.000000        0.000000      0.000000        0.000000   \n","6     0.041933        0.001118   6183.344727        0.041933   \n","7     0.000000        0.000000      0.000000        0.000000   \n","8     0.041539        0.001103  12250.351562        0.041539   \n","9     0.000000        0.000000      0.000000        0.000000   \n","10    0.034288        0.000832  20223.839844        0.034288   \n","11    0.000000        0.000000      0.000000        0.000000   \n","12    0.032159        0.000741  18968.369141        0.032159   \n","13    0.000000        0.000000      0.000000        0.000000   \n","14    0.032218        0.000737  38005.714844        0.032218   \n","15    0.000000        0.000000      0.000000        0.000000   \n","16    0.023702        0.000453  55919.343750        0.023702   \n","17    0.000000        0.000000      0.000000        0.000000   \n","18    0.020526        0.000309  48428.031250        0.020526   \n","19    0.000000        0.000000      0.000000        0.000000   \n","20    0.019337        0.000255  45621.121094        0.019337   \n","21    0.000000        0.000000      0.000000        0.000000   \n","22    0.018923        0.000234  44643.882812        0.018923   \n","23    0.000000        0.000000      0.000000        0.000000   \n","24    0.019276        0.000248  45477.464844        0.019276   \n","25    0.000000        0.000000      0.000000        0.000000   \n","26    0.049628        0.001454    254.092865        0.049628   \n","27    0.000000        0.000000      0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.026066     372.808960      True  \n","1             0.000000       0.000000     False  \n","2             0.001713    1973.927124      True  \n","3             0.000000       0.000000     False  \n","4             0.001772    4005.279297      True  \n","5             0.000000       0.000000     False  \n","6             0.001118    6183.344727      True  \n","7             0.000000       0.000000     False  \n","8             0.001103   12250.351562      True  \n","9             0.000000       0.000000     False  \n","10            0.000832   20223.839844      True  \n","11            0.000000       0.000000     False  \n","12            0.000741   18968.369141      True  \n","13            0.000000       0.000000     False  \n","14            0.000737   38005.714844      True  \n","15            0.000000       0.000000     False  \n","16            0.000453   55919.343750      True  \n","17            0.000000       0.000000     False  \n","18            0.000309   48428.031250      True  \n","19            0.000000       0.000000     False  \n","20            0.000255   45621.121094      True  \n","21            0.000000       0.000000     False  \n","22            0.000234   44643.882812      True  \n","23            0.000000       0.000000     False  \n","24            0.000248   45477.464844      True  \n","25            0.000000       0.000000     False  \n","26            0.001454     254.092865      True  \n","27            0.000000       0.000000     False  \n","Parameter Sparsity: 9289140/14719818 (0.6311)\n","FLOP Sparsity: 228891015/313478154 (0.7302)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner mag  --pre-epochs 10 --post-epochs 10 --compression 0.5 --expid mag_0_5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vge7w8ePoOxy","executionInfo":{"status":"ok","timestamp":1677449289904,"user_tz":300,"elapsed":571103,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"59454987-cb49-4479-8f10-b52a4dc45ae5"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [04:38<00:00, 27.81s/it]\n","Time_pre_train:  279.8667499640005\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.31it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:40<00:00, 28.03s/it]\n","Time_post_train:  282.0161574440008\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  10    0.636168   0.625882          78.96          98.43\n","Post-Prune 0          NaN   0.741066          76.92          97.59\n","Final      10    0.367984   0.525300          82.84          98.98\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.919560     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.683404    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.684923    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.594231   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.590013   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.500393   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.474236   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.475864  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.326959  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.272403  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.251346  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.244026  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.250995  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.660742     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance     score sum  score abs mean  \\\n","0     0.215746        0.026066    372.808960        0.215746   \n","1     0.000000        0.000000      0.000000        0.000000   \n","2     0.053546        0.001713   1973.927124        0.053546   \n","3     0.000000        0.000000      0.000000        0.000000   \n","4     0.054325        0.001772   4005.279297        0.054325   \n","5     0.000000        0.000000      0.000000        0.000000   \n","6     0.041933        0.001118   6183.344727        0.041933   \n","7     0.000000        0.000000      0.000000        0.000000   \n","8     0.041539        0.001103  12250.351562        0.041539   \n","9     0.000000        0.000000      0.000000        0.000000   \n","10    0.034288        0.000832  20223.839844        0.034288   \n","11    0.000000        0.000000      0.000000        0.000000   \n","12    0.032159        0.000741  18968.369141        0.032159   \n","13    0.000000        0.000000      0.000000        0.000000   \n","14    0.032218        0.000737  38005.714844        0.032218   \n","15    0.000000        0.000000      0.000000        0.000000   \n","16    0.023702        0.000453  55919.343750        0.023702   \n","17    0.000000        0.000000      0.000000        0.000000   \n","18    0.020526        0.000309  48428.031250        0.020526   \n","19    0.000000        0.000000      0.000000        0.000000   \n","20    0.019337        0.000255  45621.121094        0.019337   \n","21    0.000000        0.000000      0.000000        0.000000   \n","22    0.018923        0.000234  44643.882812        0.018923   \n","23    0.000000        0.000000      0.000000        0.000000   \n","24    0.019276        0.000248  45477.464844        0.019276   \n","25    0.000000        0.000000      0.000000        0.000000   \n","26    0.049628        0.001454    254.092865        0.049628   \n","27    0.000000        0.000000      0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.026066     372.808960      True  \n","1             0.000000       0.000000     False  \n","2             0.001713    1973.927124      True  \n","3             0.000000       0.000000     False  \n","4             0.001772    4005.279297      True  \n","5             0.000000       0.000000     False  \n","6             0.001118    6183.344727      True  \n","7             0.000000       0.000000     False  \n","8             0.001103   12250.351562      True  \n","9             0.000000       0.000000     False  \n","10            0.000832   20223.839844      True  \n","11            0.000000       0.000000     False  \n","12            0.000741   18968.369141      True  \n","13            0.000000       0.000000     False  \n","14            0.000737   38005.714844      True  \n","15            0.000000       0.000000     False  \n","16            0.000453   55919.343750      True  \n","17            0.000000       0.000000     False  \n","18            0.000309   48428.031250      True  \n","19            0.000000       0.000000     False  \n","20            0.000255   45621.121094      True  \n","21            0.000000       0.000000     False  \n","22            0.000234   44643.882812      True  \n","23            0.000000       0.000000     False  \n","24            0.000248   45477.464844      True  \n","25            0.000000       0.000000     False  \n","26            0.001454     254.092865      True  \n","27            0.000000       0.000000     False  \n","Parameter Sparsity: 4657707/14719818 (0.3164)\n","FLOP Sparsity: 149641174/313478154 (0.4774)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner mag  --pre-epochs 10 --post-epochs 10 --compression 1 --expid mag_1_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9r-tCeIyoTG0","executionInfo":{"status":"ok","timestamp":1677449858562,"user_tz":300,"elapsed":568664,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"93b6d25b-0670-4daa-bb92-cad803717e4f"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [04:37<00:00, 27.79s/it]\n","Time_pre_train:  279.67431144700004\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00,  6.94it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:38<00:00, 27.87s/it]\n","Time_post_train:  280.38872420999905\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  10    0.636168   0.625882          78.96          98.43\n","Post-Prune 0          NaN   2.198751          28.27          58.58\n","Final      10    0.362002   0.484451          84.35          99.06\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.865162     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.460775    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.466607    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.342041   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.336253   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.242089   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.213114   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.214671  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.106745  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.065246  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.050061  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.044524  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.049265  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.426953     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance     score sum  score abs mean  \\\n","0     0.215746        0.026066    372.808960        0.215746   \n","1     0.000000        0.000000      0.000000        0.000000   \n","2     0.053546        0.001713   1973.927124        0.053546   \n","3     0.000000        0.000000      0.000000        0.000000   \n","4     0.054325        0.001772   4005.279297        0.054325   \n","5     0.000000        0.000000      0.000000        0.000000   \n","6     0.041933        0.001118   6183.344727        0.041933   \n","7     0.000000        0.000000      0.000000        0.000000   \n","8     0.041539        0.001103  12250.351562        0.041539   \n","9     0.000000        0.000000      0.000000        0.000000   \n","10    0.034288        0.000832  20223.839844        0.034288   \n","11    0.000000        0.000000      0.000000        0.000000   \n","12    0.032159        0.000741  18968.369141        0.032159   \n","13    0.000000        0.000000      0.000000        0.000000   \n","14    0.032218        0.000737  38005.714844        0.032218   \n","15    0.000000        0.000000      0.000000        0.000000   \n","16    0.023702        0.000453  55919.343750        0.023702   \n","17    0.000000        0.000000      0.000000        0.000000   \n","18    0.020526        0.000309  48428.031250        0.020526   \n","19    0.000000        0.000000      0.000000        0.000000   \n","20    0.019337        0.000255  45621.121094        0.019337   \n","21    0.000000        0.000000      0.000000        0.000000   \n","22    0.018923        0.000234  44643.882812        0.018923   \n","23    0.000000        0.000000      0.000000        0.000000   \n","24    0.019276        0.000248  45477.464844        0.019276   \n","25    0.000000        0.000000      0.000000        0.000000   \n","26    0.049628        0.001454    254.092865        0.049628   \n","27    0.000000        0.000000      0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.026066     372.808960      True  \n","1             0.000000       0.000000     False  \n","2             0.001713    1973.927124      True  \n","3             0.000000       0.000000     False  \n","4             0.001772    4005.279297      True  \n","5             0.000000       0.000000     False  \n","6             0.001118    6183.344727      True  \n","7             0.000000       0.000000     False  \n","8             0.001103   12250.351562      True  \n","9             0.000000       0.000000     False  \n","10            0.000832   20223.839844      True  \n","11            0.000000       0.000000     False  \n","12            0.000741   18968.369141      True  \n","13            0.000000       0.000000     False  \n","14            0.000737   38005.714844      True  \n","15            0.000000       0.000000     False  \n","16            0.000453   55919.343750      True  \n","17            0.000000       0.000000     False  \n","18            0.000309   48428.031250      True  \n","19            0.000000       0.000000     False  \n","20            0.000255   45621.121094      True  \n","21            0.000000       0.000000     False  \n","22            0.000234   44643.882812      True  \n","23            0.000000       0.000000     False  \n","24            0.000248   45477.464844      True  \n","25            0.000000       0.000000     False  \n","26            0.001454     254.092865      True  \n","27            0.000000       0.000000     False  \n","Parameter Sparsity: 1475793/14719818 (0.1003)\n","FLOP Sparsity: 76353420/313478154 (0.2436)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner mag  --pre-epochs 10 --post-epochs 10 --compression 2 --expid mag_2_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0qpP0tNohPQ","executionInfo":{"status":"ok","timestamp":1677450426508,"user_tz":300,"elapsed":567950,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"84f86fb4-da2b-4e58-e510-e4b35494dc04"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 10 epochs.\n","100% 10/10 [04:38<00:00, 27.84s/it]\n","Time_pre_train:  280.0706520429994\n","Pruning with mag for 1 epochs.\n","100% 1/1 [00:00<00:00,  7.37it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:37<00:00, 27.72s/it]\n","Time_post_train:  278.8969968279998\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  10    0.636168   0.625882          78.96          98.43\n","Post-Prune 0          NaN   2.322805          10.00          50.00\n","Final      10    0.871275   0.855658          69.57          97.75\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.703125     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.126953    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.134060    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.058967   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.057081   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.031979   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.025357   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.025013  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.009391  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.003752  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.001947  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.001213  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.001500  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.101367     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","    score mean  score variance     score sum  score abs mean  \\\n","0     0.215746        0.026066    372.808960        0.215746   \n","1     0.000000        0.000000      0.000000        0.000000   \n","2     0.053546        0.001713   1973.927124        0.053546   \n","3     0.000000        0.000000      0.000000        0.000000   \n","4     0.054325        0.001772   4005.279297        0.054325   \n","5     0.000000        0.000000      0.000000        0.000000   \n","6     0.041933        0.001118   6183.344727        0.041933   \n","7     0.000000        0.000000      0.000000        0.000000   \n","8     0.041539        0.001103  12250.351562        0.041539   \n","9     0.000000        0.000000      0.000000        0.000000   \n","10    0.034288        0.000832  20223.839844        0.034288   \n","11    0.000000        0.000000      0.000000        0.000000   \n","12    0.032159        0.000741  18968.369141        0.032159   \n","13    0.000000        0.000000      0.000000        0.000000   \n","14    0.032218        0.000737  38005.714844        0.032218   \n","15    0.000000        0.000000      0.000000        0.000000   \n","16    0.023702        0.000453  55919.343750        0.023702   \n","17    0.000000        0.000000      0.000000        0.000000   \n","18    0.020526        0.000309  48428.031250        0.020526   \n","19    0.000000        0.000000      0.000000        0.000000   \n","20    0.019337        0.000255  45621.121094        0.019337   \n","21    0.000000        0.000000      0.000000        0.000000   \n","22    0.018923        0.000234  44643.882812        0.018923   \n","23    0.000000        0.000000      0.000000        0.000000   \n","24    0.019276        0.000248  45477.464844        0.019276   \n","25    0.000000        0.000000      0.000000        0.000000   \n","26    0.049628        0.001454    254.092865        0.049628   \n","27    0.000000        0.000000      0.000000        0.000000   \n","\n","    score abs variance  score abs sum  prunable  \n","0             0.026066     372.808960      True  \n","1             0.000000       0.000000     False  \n","2             0.001713    1973.927124      True  \n","3             0.000000       0.000000     False  \n","4             0.001772    4005.279297      True  \n","5             0.000000       0.000000     False  \n","6             0.001118    6183.344727      True  \n","7             0.000000       0.000000     False  \n","8             0.001103   12250.351562      True  \n","9             0.000000       0.000000     False  \n","10            0.000832   20223.839844      True  \n","11            0.000000       0.000000     False  \n","12            0.000741   18968.369141      True  \n","13            0.000000       0.000000     False  \n","14            0.000737   38005.714844      True  \n","15            0.000000       0.000000     False  \n","16            0.000453   55919.343750      True  \n","17            0.000000       0.000000     False  \n","18            0.000309   48428.031250      True  \n","19            0.000000       0.000000     False  \n","20            0.000255   45621.121094      True  \n","21            0.000000       0.000000     False  \n","22            0.000234   44643.882812      True  \n","23            0.000000       0.000000     False  \n","24            0.000248   45477.464844      True  \n","25            0.000000       0.000000     False  \n","26            0.001454     254.092865      True  \n","27            0.000000       0.000000     False  \n","Parameter Sparsity: 151389/14719818 (0.0103)\n","FLOP Sparsity: 15323660/313478154 (0.0489)\n","Saving results.\n"]}]},{"cell_type":"markdown","source":["SNIP"],"metadata":{"id":"YFFT4vYFopO_"}},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner snip --post-epochs 10 --compression 0.05 --expid snip_0_05"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VygMiM8CqDRQ","executionInfo":{"status":"ok","timestamp":1677466754704,"user_tz":300,"elapsed":315860,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"58fadf17-2a8f-4bf8-e730-3b7c3dbbefd0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment 'singleshot' with expid 'snip_0_05' exists.  Overwrite (yes/no)? yes\n","Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.6435045580001315\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.87it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:21<00:00, 26.18s/it]\n","Time_post_train:  263.330405769\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.417982          11.82          50.10\n","Final      10    0.587501   0.630510          79.04          98.34\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.999973    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.999932    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.997314   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.992482   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.980977   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.977198   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.961445  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.931232  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.939174  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.884741  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.790958  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.806471  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.986328     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   2.148237e-06    8.221934e-12   0.003712    2.148237e-06   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   4.372236e-07    4.510187e-13   0.016118    4.372236e-07   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   3.024112e-07    3.032936e-13   0.022296    3.024112e-07   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   1.802230e-07    1.371213e-13   0.026575    1.802230e-07   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   1.432872e-07    8.967040e-14   0.042257    1.432872e-07   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  8.978097e-08    3.946946e-14   0.052955    8.978097e-08   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  9.579716e-08    4.068194e-14   0.056503    9.579716e-08   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  8.542810e-08    3.210387e-14   0.100775    8.542810e-08   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  5.670231e-08    1.667407e-14   0.133778    5.670231e-08   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  5.675370e-08    1.527773e-14   0.133899    5.675370e-08   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  6.344949e-08    2.087360e-14   0.149696    6.344949e-08   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  5.212994e-08    2.199752e-14   0.122990    5.212994e-08   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  5.339835e-08    2.426836e-14   0.125983    5.339835e-08   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  2.434234e-06    1.646431e-11   0.012463    2.434234e-06   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         8.221934e-12       0.003712      True  \n","1         0.000000e+00       0.000000     False  \n","2         4.510187e-13       0.016118      True  \n","3         0.000000e+00       0.000000     False  \n","4         3.032936e-13       0.022296      True  \n","5         0.000000e+00       0.000000     False  \n","6         1.371213e-13       0.026575      True  \n","7         0.000000e+00       0.000000     False  \n","8         8.967040e-14       0.042257      True  \n","9         0.000000e+00       0.000000     False  \n","10        3.946946e-14       0.052955      True  \n","11        0.000000e+00       0.000000     False  \n","12        4.068194e-14       0.056503      True  \n","13        0.000000e+00       0.000000     False  \n","14        3.210387e-14       0.100775      True  \n","15        0.000000e+00       0.000000     False  \n","16        1.667407e-14       0.133778      True  \n","17        0.000000e+00       0.000000     False  \n","18        1.527773e-14       0.133899      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.087360e-14       0.149696      True  \n","21        0.000000e+00       0.000000     False  \n","22        2.199752e-14       0.122990      True  \n","23        0.000000e+00       0.000000     False  \n","24        2.426836e-14       0.125983      True  \n","25        0.000000e+00       0.000000     False  \n","26        1.646431e-11       0.012463      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 13119513/14719818 (0.8913)\n","FLOP Sparsity: 301147116/313478154 (0.9607)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner snip --post-epochs 10 --compression 0.1 --expid snip_0_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bg0gsEzFqOdY","executionInfo":{"status":"ok","timestamp":1677451018291,"user_tz":300,"elapsed":271220,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"b0554873-9dbc-430b-bd9d-6adb43df30d3"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.765789990000485\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.73it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:38<00:00, 27.87s/it]\n","Time_post_train:  280.4749943480001\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.425629          12.14          50.10\n","Final      10     0.57676   0.633233          78.49          98.22\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.999403    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.998074    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.992072   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.982863   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.963762   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.958850   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.936335  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.894231  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.888944  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.756644  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.614106  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.617162  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.985547     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   2.148237e-06    8.221934e-12   0.003712    2.148237e-06   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   4.372236e-07    4.510187e-13   0.016118    4.372236e-07   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   3.024112e-07    3.032936e-13   0.022296    3.024112e-07   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   1.802230e-07    1.371213e-13   0.026575    1.802230e-07   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   1.432872e-07    8.967040e-14   0.042257    1.432872e-07   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  8.978097e-08    3.946946e-14   0.052955    8.978097e-08   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  9.579716e-08    4.068194e-14   0.056503    9.579716e-08   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  8.542810e-08    3.210387e-14   0.100775    8.542810e-08   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  5.670231e-08    1.667407e-14   0.133778    5.670231e-08   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  5.675370e-08    1.527773e-14   0.133899    5.675370e-08   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  6.344949e-08    2.087360e-14   0.149696    6.344949e-08   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  5.212994e-08    2.199752e-14   0.122990    5.212994e-08   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  5.339835e-08    2.426836e-14   0.125983    5.339835e-08   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  2.434234e-06    1.646431e-11   0.012463    2.434234e-06   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         8.221934e-12       0.003712      True  \n","1         0.000000e+00       0.000000     False  \n","2         4.510187e-13       0.016118      True  \n","3         0.000000e+00       0.000000     False  \n","4         3.032936e-13       0.022296      True  \n","5         0.000000e+00       0.000000     False  \n","6         1.371213e-13       0.026575      True  \n","7         0.000000e+00       0.000000     False  \n","8         8.967040e-14       0.042257      True  \n","9         0.000000e+00       0.000000     False  \n","10        3.946946e-14       0.052955      True  \n","11        0.000000e+00       0.000000     False  \n","12        4.068194e-14       0.056503      True  \n","13        0.000000e+00       0.000000     False  \n","14        3.210387e-14       0.100775      True  \n","15        0.000000e+00       0.000000     False  \n","16        1.667407e-14       0.133778      True  \n","17        0.000000e+00       0.000000     False  \n","18        1.527773e-14       0.133899      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.087360e-14       0.149696      True  \n","21        0.000000e+00       0.000000     False  \n","22        2.199752e-14       0.122990      True  \n","23        0.000000e+00       0.000000     False  \n","24        2.426836e-14       0.125983      True  \n","25        0.000000e+00       0.000000     False  \n","26        1.646431e-11       0.012463      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 11693237/14719818 (0.7944)\n","FLOP Sparsity: 290937408/313478154 (0.9281)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner snip --post-epochs 10 --compression 0.2 --expid snip_0_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SDd6fwbAqT-B","executionInfo":{"status":"ok","timestamp":1677451317234,"user_tz":300,"elapsed":298586,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"0db488e3-cb4b-4cd8-ea0f-29a2e63922a9"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7599536940015241\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.59it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:46<00:00, 28.67s/it]\n","Time_post_train:  288.38680848900003\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.436287          10.16          51.57\n","Final      10    0.614026   0.646890          77.95          98.42\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.995370     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.970812    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.935425    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.872321   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.841292   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.769940   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.793133   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.775036  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.677798  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.694555  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.613605  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.484222  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.480073  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.970312     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   2.148237e-06    8.221934e-12   0.003712    2.148237e-06   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   4.372236e-07    4.510187e-13   0.016118    4.372236e-07   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   3.024112e-07    3.032936e-13   0.022296    3.024112e-07   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   1.802230e-07    1.371213e-13   0.026575    1.802230e-07   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   1.432872e-07    8.967040e-14   0.042257    1.432872e-07   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  8.978097e-08    3.946946e-14   0.052955    8.978097e-08   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  9.579716e-08    4.068194e-14   0.056503    9.579716e-08   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  8.542810e-08    3.210387e-14   0.100775    8.542810e-08   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  5.670231e-08    1.667407e-14   0.133778    5.670231e-08   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  5.675370e-08    1.527773e-14   0.133899    5.675370e-08   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  6.344949e-08    2.087360e-14   0.149696    6.344949e-08   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  5.212994e-08    2.199752e-14   0.122990    5.212994e-08   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  5.339835e-08    2.426836e-14   0.125983    5.339835e-08   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  2.434234e-06    1.646431e-11   0.012463    2.434234e-06   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         8.221934e-12       0.003712      True  \n","1         0.000000e+00       0.000000     False  \n","2         4.510187e-13       0.016118      True  \n","3         0.000000e+00       0.000000     False  \n","4         3.032936e-13       0.022296      True  \n","5         0.000000e+00       0.000000     False  \n","6         1.371213e-13       0.026575      True  \n","7         0.000000e+00       0.000000     False  \n","8         8.967040e-14       0.042257      True  \n","9         0.000000e+00       0.000000     False  \n","10        3.946946e-14       0.052955      True  \n","11        0.000000e+00       0.000000     False  \n","12        4.068194e-14       0.056503      True  \n","13        0.000000e+00       0.000000     False  \n","14        3.210387e-14       0.100775      True  \n","15        0.000000e+00       0.000000     False  \n","16        1.667407e-14       0.133778      True  \n","17        0.000000e+00       0.000000     False  \n","18        1.527773e-14       0.133899      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.087360e-14       0.149696      True  \n","21        0.000000e+00       0.000000     False  \n","22        2.199752e-14       0.122990      True  \n","23        0.000000e+00       0.000000     False  \n","24        2.426836e-14       0.125983      True  \n","25        0.000000e+00       0.000000     False  \n","26        1.646431e-11       0.012463      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 9289139/14719818 (0.6311)\n","FLOP Sparsity: 245480933/313478154 (0.7831)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner snip --post-epochs 10 --compression 0.5 --expid snip_0_5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f09jHiVrqXsZ","executionInfo":{"status":"ok","timestamp":1677467034326,"user_tz":300,"elapsed":279630,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"bde78326-9379-4fbe-e0f7-76384c118e82"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7213288319999265\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.41it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:26<00:00, 26.66s/it]\n","Time_post_train:  268.27017061599986\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.440385           8.65          50.82\n","Final      10    0.578925   0.635512          78.46          98.06\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.961227     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.810954    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.696438    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.557475   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.497128   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.393863   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.428433   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.405871  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.308566  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.317802  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.319192  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.241640  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.242627  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.894727     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   2.148237e-06    8.221934e-12   0.003712    2.148237e-06   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   4.372236e-07    4.510187e-13   0.016118    4.372236e-07   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   3.024112e-07    3.032936e-13   0.022296    3.024112e-07   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   1.802230e-07    1.371213e-13   0.026575    1.802230e-07   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   1.432872e-07    8.967040e-14   0.042257    1.432872e-07   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  8.978097e-08    3.946946e-14   0.052955    8.978097e-08   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  9.579716e-08    4.068194e-14   0.056503    9.579716e-08   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  8.542810e-08    3.210387e-14   0.100775    8.542810e-08   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  5.670231e-08    1.667407e-14   0.133778    5.670231e-08   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  5.675370e-08    1.527773e-14   0.133899    5.675370e-08   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  6.344949e-08    2.087360e-14   0.149696    6.344949e-08   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  5.212994e-08    2.199752e-14   0.122990    5.212994e-08   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  5.339835e-08    2.426836e-14   0.125983    5.339835e-08   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  2.434234e-06    1.646431e-11   0.012463    2.434234e-06   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         8.221934e-12       0.003712      True  \n","1         0.000000e+00       0.000000     False  \n","2         4.510187e-13       0.016118      True  \n","3         0.000000e+00       0.000000     False  \n","4         3.032936e-13       0.022296      True  \n","5         0.000000e+00       0.000000     False  \n","6         1.371213e-13       0.026575      True  \n","7         0.000000e+00       0.000000     False  \n","8         8.967040e-14       0.042257      True  \n","9         0.000000e+00       0.000000     False  \n","10        3.946946e-14       0.052955      True  \n","11        0.000000e+00       0.000000     False  \n","12        4.068194e-14       0.056503      True  \n","13        0.000000e+00       0.000000     False  \n","14        3.210387e-14       0.100775      True  \n","15        0.000000e+00       0.000000     False  \n","16        1.667407e-14       0.133778      True  \n","17        0.000000e+00       0.000000     False  \n","18        1.527773e-14       0.133899      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.087360e-14       0.149696      True  \n","21        0.000000e+00       0.000000     False  \n","22        2.199752e-14       0.122990      True  \n","23        0.000000e+00       0.000000     False  \n","24        2.426836e-14       0.125983      True  \n","25        0.000000e+00       0.000000     False  \n","26        1.646431e-11       0.012463      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 4657711/14719818 (0.3164)\n","FLOP Sparsity: 146094392/313478154 (0.4660)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner snip --post-epochs 10 --compression 1 --expid snip_1_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6oaY1ryqaqH","executionInfo":{"status":"ok","timestamp":1677451918379,"user_tz":300,"elapsed":300092,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"e9ac0bba-708e-446f-a1da-08ca306c6e5f"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7672657820003224\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.73it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:47<00:00, 28.79s/it]\n","Time_post_train:  289.61662312099907\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.322194           9.76          50.66\n","Final      10    0.656774   0.664631          76.98          98.17\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.858796     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.520616    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.381497    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.252489   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.207747   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.135130   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.146437   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.130799  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.082426  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.082548  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.099391  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.079108  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.080398  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.751758     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   2.148237e-06    8.221934e-12   0.003712    2.148237e-06   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   4.372236e-07    4.510187e-13   0.016118    4.372236e-07   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   3.024112e-07    3.032936e-13   0.022296    3.024112e-07   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   1.802230e-07    1.371213e-13   0.026575    1.802230e-07   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   1.432872e-07    8.967040e-14   0.042257    1.432872e-07   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  8.978097e-08    3.946946e-14   0.052955    8.978097e-08   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  9.579716e-08    4.068194e-14   0.056503    9.579716e-08   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  8.542810e-08    3.210387e-14   0.100775    8.542810e-08   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  5.670231e-08    1.667407e-14   0.133778    5.670231e-08   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  5.675370e-08    1.527773e-14   0.133899    5.675370e-08   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  6.344949e-08    2.087360e-14   0.149696    6.344949e-08   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  5.212994e-08    2.199752e-14   0.122990    5.212994e-08   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  5.339835e-08    2.426836e-14   0.125983    5.339835e-08   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  2.434234e-06    1.646431e-11   0.012463    2.434234e-06   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         8.221934e-12       0.003712      True  \n","1         0.000000e+00       0.000000     False  \n","2         4.510187e-13       0.016118      True  \n","3         0.000000e+00       0.000000     False  \n","4         3.032936e-13       0.022296      True  \n","5         0.000000e+00       0.000000     False  \n","6         1.371213e-13       0.026575      True  \n","7         0.000000e+00       0.000000     False  \n","8         8.967040e-14       0.042257      True  \n","9         0.000000e+00       0.000000     False  \n","10        3.946946e-14       0.052955      True  \n","11        0.000000e+00       0.000000     False  \n","12        4.068194e-14       0.056503      True  \n","13        0.000000e+00       0.000000     False  \n","14        3.210387e-14       0.100775      True  \n","15        0.000000e+00       0.000000     False  \n","16        1.667407e-14       0.133778      True  \n","17        0.000000e+00       0.000000     False  \n","18        1.527773e-14       0.133899      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.087360e-14       0.149696      True  \n","21        0.000000e+00       0.000000     False  \n","22        2.199752e-14       0.122990      True  \n","23        0.000000e+00       0.000000     False  \n","24        2.426836e-14       0.125983      True  \n","25        0.000000e+00       0.000000     False  \n","26        1.646431e-11       0.012463      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 1475792/14719818 (0.1003)\n","FLOP Sparsity: 63873673/313478154 (0.2038)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner snip --post-epochs 10 --compression 2 --expid snip_2_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1Hm25qkqer7","executionInfo":{"status":"ok","timestamp":1677467308501,"user_tz":300,"elapsed":274186,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"ab9ca18b-fee7-44c3-ecf4-fd07c7d97712"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.708274996\n","Pruning with snip for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.76it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:21<00:00, 26.17s/it]\n","Time_post_train:  263.242202377\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302597          10.00          50.00\n","Final      10    1.223153   1.168915          57.25          95.15\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.596644     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.161974    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.102010    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.049540   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.035709   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.015686   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.015789   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.012149  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.005388  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.004678  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.007243  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.007855  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.008448  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.508008     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   2.148237e-06    8.221934e-12   0.003712    2.148237e-06   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   4.372236e-07    4.510187e-13   0.016118    4.372236e-07   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   3.024112e-07    3.032936e-13   0.022296    3.024112e-07   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   1.802230e-07    1.371213e-13   0.026575    1.802230e-07   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   1.432872e-07    8.967040e-14   0.042257    1.432872e-07   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  8.978097e-08    3.946946e-14   0.052955    8.978097e-08   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  9.579716e-08    4.068194e-14   0.056503    9.579716e-08   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  8.542810e-08    3.210387e-14   0.100775    8.542810e-08   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  5.670231e-08    1.667407e-14   0.133778    5.670231e-08   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  5.675370e-08    1.527773e-14   0.133899    5.675370e-08   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  6.344949e-08    2.087360e-14   0.149696    6.344949e-08   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  5.212994e-08    2.199752e-14   0.122990    5.212994e-08   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  5.339835e-08    2.426836e-14   0.125983    5.339835e-08   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  2.434234e-06    1.646431e-11   0.012463    2.434234e-06   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         8.221934e-12       0.003712      True  \n","1         0.000000e+00       0.000000     False  \n","2         4.510187e-13       0.016118      True  \n","3         0.000000e+00       0.000000     False  \n","4         3.032936e-13       0.022296      True  \n","5         0.000000e+00       0.000000     False  \n","6         1.371213e-13       0.026575      True  \n","7         0.000000e+00       0.000000     False  \n","8         8.967040e-14       0.042257      True  \n","9         0.000000e+00       0.000000     False  \n","10        3.946946e-14       0.052955      True  \n","11        0.000000e+00       0.000000     False  \n","12        4.068194e-14       0.056503      True  \n","13        0.000000e+00       0.000000     False  \n","14        3.210387e-14       0.100775      True  \n","15        0.000000e+00       0.000000     False  \n","16        1.667407e-14       0.133778      True  \n","17        0.000000e+00       0.000000     False  \n","18        1.527773e-14       0.133899      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.087360e-14       0.149696      True  \n","21        0.000000e+00       0.000000     False  \n","22        2.199752e-14       0.122990      True  \n","23        0.000000e+00       0.000000     False  \n","24        2.426836e-14       0.125983      True  \n","25        0.000000e+00       0.000000     False  \n","26        1.646431e-11       0.012463      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 151390/14719818 (0.0103)\n","FLOP Sparsity: 13938223/313478154 (0.0445)\n","Saving results.\n"]}]},{"cell_type":"markdown","source":["GraSP"],"metadata":{"id":"RC772CifrH1J"}},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner grasp --post-epochs 10 --compression 0.05 --expid grasp_0_05"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWhAKEXbrJsC","executionInfo":{"status":"ok","timestamp":1677468610444,"user_tz":300,"elapsed":1301945,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"773650cd-7d92-483d-9ec7-77f753b072b6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment 'singleshot' with expid 'grasp_0_05' exists.  Overwrite (yes/no)? yes\n","Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.617791478000072\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:01<00:00,  1.61s/it]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:21<00:00, 26.11s/it]\n","Time_post_train:  262.67488427499984\n","Train results:\n","                train_loss     test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN  2.417717e+00          11.73          50.17\n","Pre-Prune  0          NaN  2.417717e+00          11.73          50.17\n","Post-Prune 0          NaN  1.022474e+10          10.00          50.00\n","Final      10    2.145926  2.057571e+00          24.71          75.70\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.584491     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.625488    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.692939    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.773953   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.802619   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.849909   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.845615   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.866002  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.909686  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.904480  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.903085  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.906107  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.896840  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.619922     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   4.450945e-05    8.900291e-08   0.076912    1.538199e-04   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   2.075134e-06    4.294618e-09   0.076498    2.520945e-05   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   1.038030e-06    1.728651e-09   0.076532    1.590854e-05   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   5.169493e-07    5.200354e-10   0.076227    7.789511e-06   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   2.561208e-07    2.032912e-10   0.075533    4.886558e-06   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  1.238564e-07    5.129472e-11   0.073054    2.461394e-06   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  1.238643e-07    3.226695e-11   0.073058    2.103923e-06   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  6.004534e-08    1.234029e-11   0.070832    1.425750e-06   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  2.802450e-08    3.534325e-12   0.066118    7.976792e-07   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  2.819010e-08    2.922252e-12   0.066509    7.932248e-07   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  2.725771e-08    3.176401e-12   0.064309    7.930591e-07   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  2.784332e-08    3.840922e-12   0.065691    7.952294e-07   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  2.729167e-08    4.988742e-12   0.064389    9.026103e-07   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  1.451916e-05    4.613413e-09   0.074338    3.635621e-05   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         6.732343e-08       0.265801      True  \n","1         0.000000e+00       0.000000     False  \n","2         3.663408e-09       0.929321      True  \n","3         0.000000e+00       0.000000     False  \n","4         1.476646e-09       1.172905      True  \n","5         0.000000e+00       0.000000     False  \n","6         4.596262e-10       1.148610      True  \n","7         0.000000e+00       0.000000     False  \n","8         1.794783e-10       1.441105      True  \n","9         0.000000e+00       0.000000     False  \n","10        4.525161e-11       1.451789      True  \n","11        0.000000e+00       0.000000     False  \n","12        2.785579e-11       1.240944      True  \n","13        0.000000e+00       0.000000     False  \n","14        1.031113e-11       1.681883      True  \n","15        0.000000e+00       0.000000     False  \n","16        2.898821e-12       1.881961      True  \n","17        0.000000e+00       0.000000     False  \n","18        2.293842e-12       1.871452      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.548202e-12       1.871061      True  \n","21        0.000000e+00       0.000000     False  \n","22        3.209310e-12       1.876181      True  \n","23        0.000000e+00       0.000000     False  \n","24        4.174782e-12       2.129525      True  \n","25        0.000000e+00       0.000000     False  \n","26        3.502445e-09       0.186144      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 13119512/14719818 (0.8913)\n","FLOP Sparsity: 256737685/313478154 (0.8190)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner grasp --post-epochs 10 --compression 0.1 --expid grasp_0_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYkTGTYZrNnu","executionInfo":{"status":"ok","timestamp":1677468888626,"user_tz":300,"elapsed":278185,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"ce6a273e-5497-40b7-ceff-e5e63f6f4a28"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7150908349999554\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:01<00:00,  1.71s/it]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:23<00:00, 26.37s/it]\n","Time_post_train:  265.34127996999996\n","Train results:\n","                train_loss     test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN  2.417717e+00          11.73          50.17\n","Pre-Prune  0          NaN  2.417717e+00          11.73          50.17\n","Post-Prune 0          NaN  2.617180e+10          10.00          50.00\n","Final      10    2.034689  1.987096e+00          28.22          79.50\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.571759     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.562500    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.619982    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.690891   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.717434   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.764618   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.747462   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.760207  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.806321  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.789907  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.799711  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.823101  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.814539  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.598828     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   4.450945e-05    8.900291e-08   0.076912    1.538199e-04   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   2.075134e-06    4.294618e-09   0.076498    2.520945e-05   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   1.038030e-06    1.728651e-09   0.076532    1.590854e-05   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   5.169493e-07    5.200354e-10   0.076227    7.789511e-06   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   2.561208e-07    2.032912e-10   0.075533    4.886558e-06   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  1.238564e-07    5.129472e-11   0.073054    2.461394e-06   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  1.238643e-07    3.226695e-11   0.073058    2.103923e-06   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  6.004534e-08    1.234029e-11   0.070832    1.425750e-06   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  2.802450e-08    3.534325e-12   0.066118    7.976792e-07   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  2.819010e-08    2.922252e-12   0.066509    7.932248e-07   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  2.725771e-08    3.176401e-12   0.064309    7.930591e-07   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  2.784332e-08    3.840922e-12   0.065691    7.952294e-07   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  2.729167e-08    4.988742e-12   0.064389    9.026103e-07   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  1.451916e-05    4.613413e-09   0.074338    3.635621e-05   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         6.732343e-08       0.265801      True  \n","1         0.000000e+00       0.000000     False  \n","2         3.663408e-09       0.929321      True  \n","3         0.000000e+00       0.000000     False  \n","4         1.476646e-09       1.172905      True  \n","5         0.000000e+00       0.000000     False  \n","6         4.596262e-10       1.148610      True  \n","7         0.000000e+00       0.000000     False  \n","8         1.794783e-10       1.441105      True  \n","9         0.000000e+00       0.000000     False  \n","10        4.525161e-11       1.451789      True  \n","11        0.000000e+00       0.000000     False  \n","12        2.785579e-11       1.240944      True  \n","13        0.000000e+00       0.000000     False  \n","14        1.031113e-11       1.681883      True  \n","15        0.000000e+00       0.000000     False  \n","16        2.898821e-12       1.881961      True  \n","17        0.000000e+00       0.000000     False  \n","18        2.293842e-12       1.871452      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.548202e-12       1.871061      True  \n","21        0.000000e+00       0.000000     False  \n","22        3.209310e-12       1.876181      True  \n","23        0.000000e+00       0.000000     False  \n","24        4.174782e-12       2.129525      True  \n","25        0.000000e+00       0.000000     False  \n","26        3.502445e-09       0.186144      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 11693238/14719818 (0.7944)\n","FLOP Sparsity: 228532943/313478154 (0.7290)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner grasp --post-epochs 10 --compression 0.2 --expid grasp_0_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dwQwRw4rT50","executionInfo":{"status":"ok","timestamp":1677469166675,"user_tz":300,"elapsed":278070,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"dc1ec2a0-089d-45c6-fe29-862ad2bf1db7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7756182800003444\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:01<00:00,  1.73s/it]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:23<00:00, 26.39s/it]\n","Time_post_train:  265.52804374699963\n","Train results:\n","                train_loss     test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN  2.417717e+00          11.73          50.17\n","Pre-Prune  0          NaN  2.417717e+00          11.73          50.17\n","Post-Prune 0          NaN  3.719964e+10          10.00          50.00\n","Final      10    2.121719  2.058512e+00          25.06          75.62\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.563079     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.507731    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.524414    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.546414   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.553392   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.576621   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.563727   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.571614  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.595104  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.588392  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.642397  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.705736  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.703603  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.586719     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   4.450945e-05    8.900291e-08   0.076912    1.538199e-04   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   2.075134e-06    4.294618e-09   0.076498    2.520945e-05   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   1.038030e-06    1.728651e-09   0.076532    1.590854e-05   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   5.169493e-07    5.200354e-10   0.076227    7.789511e-06   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   2.561208e-07    2.032912e-10   0.075533    4.886558e-06   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  1.238564e-07    5.129472e-11   0.073054    2.461394e-06   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  1.238643e-07    3.226695e-11   0.073058    2.103923e-06   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  6.004534e-08    1.234029e-11   0.070832    1.425750e-06   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  2.802450e-08    3.534325e-12   0.066118    7.976792e-07   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  2.819010e-08    2.922252e-12   0.066509    7.932248e-07   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  2.725771e-08    3.176401e-12   0.064309    7.930591e-07   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  2.784332e-08    3.840922e-12   0.065691    7.952294e-07   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  2.729167e-08    4.988742e-12   0.064389    9.026103e-07   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  1.451916e-05    4.613413e-09   0.074338    3.635621e-05   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         6.732343e-08       0.265801      True  \n","1         0.000000e+00       0.000000     False  \n","2         3.663408e-09       0.929321      True  \n","3         0.000000e+00       0.000000     False  \n","4         1.476646e-09       1.172905      True  \n","5         0.000000e+00       0.000000     False  \n","6         4.596262e-10       1.148610      True  \n","7         0.000000e+00       0.000000     False  \n","8         1.794783e-10       1.441105      True  \n","9         0.000000e+00       0.000000     False  \n","10        4.525161e-11       1.451789      True  \n","11        0.000000e+00       0.000000     False  \n","12        2.785579e-11       1.240944      True  \n","13        0.000000e+00       0.000000     False  \n","14        1.031113e-11       1.681883      True  \n","15        0.000000e+00       0.000000     False  \n","16        2.898821e-12       1.881961      True  \n","17        0.000000e+00       0.000000     False  \n","18        2.293842e-12       1.871452      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.548202e-12       1.871061      True  \n","21        0.000000e+00       0.000000     False  \n","22        3.209310e-12       1.876181      True  \n","23        0.000000e+00       0.000000     False  \n","24        4.174782e-12       2.129525      True  \n","25        0.000000e+00       0.000000     False  \n","26        3.502445e-09       0.186144      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 9289139/14719818 (0.6311)\n","FLOP Sparsity: 179285021/313478154 (0.5719)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner grasp --post-epochs 10 --compression 0.5 --expid grasp_0_5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGc235fSrYGq","executionInfo":{"status":"ok","timestamp":1677469445655,"user_tz":300,"elapsed":278988,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"f89734cf-5441-4eeb-bdf2-48e36fe73c9d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.6932302879995405\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:01<00:00,  1.74s/it]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:24<00:00, 26.46s/it]\n","Time_post_train:  266.2103456959994\n","Train results:\n","                train_loss     test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN  2.417717e+00          11.73          50.17\n","Pre-Prune  0          NaN  2.417717e+00          11.73          50.17\n","Post-Prune 0          NaN  4.021371e+10          10.00          50.00\n","Final      10    2.150575  2.101732e+00          23.18          73.65\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.560185     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.483181    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.455471    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.407010   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.384271   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.349486   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.371540   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.364069  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.328578  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.346785  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.311515  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.260431  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.265902  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.568359     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   4.450945e-05    8.900291e-08   0.076912    1.538199e-04   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   2.075134e-06    4.294618e-09   0.076498    2.520945e-05   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   1.038030e-06    1.728651e-09   0.076532    1.590854e-05   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   5.169493e-07    5.200354e-10   0.076227    7.789511e-06   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   2.561208e-07    2.032912e-10   0.075533    4.886558e-06   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  1.238564e-07    5.129472e-11   0.073054    2.461394e-06   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  1.238643e-07    3.226695e-11   0.073058    2.103923e-06   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  6.004534e-08    1.234029e-11   0.070832    1.425750e-06   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  2.802450e-08    3.534325e-12   0.066118    7.976792e-07   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  2.819010e-08    2.922252e-12   0.066509    7.932248e-07   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  2.725771e-08    3.176401e-12   0.064309    7.930591e-07   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  2.784332e-08    3.840922e-12   0.065691    7.952294e-07   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  2.729167e-08    4.988742e-12   0.064389    9.026103e-07   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  1.451916e-05    4.613413e-09   0.074338    3.635621e-05   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         6.732343e-08       0.265801      True  \n","1         0.000000e+00       0.000000     False  \n","2         3.663408e-09       0.929321      True  \n","3         0.000000e+00       0.000000     False  \n","4         1.476646e-09       1.172905      True  \n","5         0.000000e+00       0.000000     False  \n","6         4.596262e-10       1.148610      True  \n","7         0.000000e+00       0.000000     False  \n","8         1.794783e-10       1.441105      True  \n","9         0.000000e+00       0.000000     False  \n","10        4.525161e-11       1.451789      True  \n","11        0.000000e+00       0.000000     False  \n","12        2.785579e-11       1.240944      True  \n","13        0.000000e+00       0.000000     False  \n","14        1.031113e-11       1.681883      True  \n","15        0.000000e+00       0.000000     False  \n","16        2.898821e-12       1.881961      True  \n","17        0.000000e+00       0.000000     False  \n","18        2.293842e-12       1.871452      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.548202e-12       1.871061      True  \n","21        0.000000e+00       0.000000     False  \n","22        3.209310e-12       1.876181      True  \n","23        0.000000e+00       0.000000     False  \n","24        4.174782e-12       2.129525      True  \n","25        0.000000e+00       0.000000     False  \n","26        3.502445e-09       0.186144      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 4657710/14719818 (0.3164)\n","FLOP Sparsity: 118214198/313478154 (0.3771)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner grasp --post-epochs 10 --compression 1 --expid grasp_1_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kemt_V1craQ1","executionInfo":{"status":"ok","timestamp":1677469722870,"user_tz":300,"elapsed":277229,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"caef160f-818d-4582-8b93-904985ba6b3a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.6690814080002383\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:01<00:00,  1.72s/it]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:23<00:00, 26.35s/it]\n","Time_post_train:  265.13871984099933\n","Train results:\n","                 train_loss     test_loss  top1_accuracy  top5_accuracy\n","Init.      0           NaN  2.417717e+00          11.73          50.17\n","Pre-Prune  0           NaN  2.417717e+00          11.73          50.17\n","Post-Prune 0           NaN  7.798163e+09          10.00          50.00\n","Final      10  5169.130754  6.594397e+03          11.18          52.30\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.538194     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.370334    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.307170    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.223050   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.190138   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.143294   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.146985   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.124977  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.081373  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.085475  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.087296  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.085676  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.094220  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.526758     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   4.450945e-05    8.900291e-08   0.076912    1.538199e-04   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   2.075134e-06    4.294618e-09   0.076498    2.520945e-05   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   1.038030e-06    1.728651e-09   0.076532    1.590854e-05   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   5.169493e-07    5.200354e-10   0.076227    7.789511e-06   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   2.561208e-07    2.032912e-10   0.075533    4.886558e-06   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  1.238564e-07    5.129472e-11   0.073054    2.461394e-06   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  1.238643e-07    3.226695e-11   0.073058    2.103923e-06   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  6.004534e-08    1.234029e-11   0.070832    1.425750e-06   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  2.802450e-08    3.534325e-12   0.066118    7.976792e-07   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  2.819010e-08    2.922252e-12   0.066509    7.932248e-07   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  2.725771e-08    3.176401e-12   0.064309    7.930591e-07   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  2.784332e-08    3.840922e-12   0.065691    7.952294e-07   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  2.729167e-08    4.988742e-12   0.064389    9.026103e-07   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  1.451916e-05    4.613413e-09   0.074338    3.635621e-05   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         6.732343e-08       0.265801      True  \n","1         0.000000e+00       0.000000     False  \n","2         3.663408e-09       0.929321      True  \n","3         0.000000e+00       0.000000     False  \n","4         1.476646e-09       1.172905      True  \n","5         0.000000e+00       0.000000     False  \n","6         4.596262e-10       1.148610      True  \n","7         0.000000e+00       0.000000     False  \n","8         1.794783e-10       1.441105      True  \n","9         0.000000e+00       0.000000     False  \n","10        4.525161e-11       1.451789      True  \n","11        0.000000e+00       0.000000     False  \n","12        2.785579e-11       1.240944      True  \n","13        0.000000e+00       0.000000     False  \n","14        1.031113e-11       1.681883      True  \n","15        0.000000e+00       0.000000     False  \n","16        2.898821e-12       1.881961      True  \n","17        0.000000e+00       0.000000     False  \n","18        2.293842e-12       1.871452      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.548202e-12       1.871061      True  \n","21        0.000000e+00       0.000000     False  \n","22        3.209310e-12       1.876181      True  \n","23        0.000000e+00       0.000000     False  \n","24        4.174782e-12       2.129525      True  \n","25        0.000000e+00       0.000000     False  \n","26        3.502445e-09       0.186144      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 1475793/14719818 (0.1003)\n","FLOP Sparsity: 55153682/313478154 (0.1759)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner grasp --post-epochs 10 --compression 2 --expid grasp_2_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NwiccfJsreIG","executionInfo":{"status":"ok","timestamp":1677469997328,"user_tz":300,"elapsed":274477,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"712b13f3-abea-4ed9-bc8e-be931442530b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.6943323350005812\n","Pruning with grasp for 1 epochs.\n","100% 1/1 [00:01<00:00,  1.70s/it]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:19<00:00, 25.95s/it]\n","Time_post_train:  261.02078097699996\n","Train results:\n","                train_loss      test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN       2.417717          11.73          50.17\n","Pre-Prune  0          NaN       2.417717          11.73          50.17\n","Post-Prune 0          NaN  108912.180713          10.00          50.00\n","Final      10    2.354707       2.307903          17.17          61.18\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.461806     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.192980    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.149224    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.089959   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.064501   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.034132   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.027103   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.013746  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.003515  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.002362  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.002667  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.003860  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.005250  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.384375     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance  score sum  score abs mean  \\\n","0   4.450945e-05    8.900291e-08   0.076912    1.538199e-04   \n","1   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","2   2.075134e-06    4.294618e-09   0.076498    2.520945e-05   \n","3   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","4   1.038030e-06    1.728651e-09   0.076532    1.590854e-05   \n","5   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","6   5.169493e-07    5.200354e-10   0.076227    7.789511e-06   \n","7   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","8   2.561208e-07    2.032912e-10   0.075533    4.886558e-06   \n","9   0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","10  1.238564e-07    5.129472e-11   0.073054    2.461394e-06   \n","11  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","12  1.238643e-07    3.226695e-11   0.073058    2.103923e-06   \n","13  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","14  6.004534e-08    1.234029e-11   0.070832    1.425750e-06   \n","15  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","16  2.802450e-08    3.534325e-12   0.066118    7.976792e-07   \n","17  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","18  2.819010e-08    2.922252e-12   0.066509    7.932248e-07   \n","19  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","20  2.725771e-08    3.176401e-12   0.064309    7.930591e-07   \n","21  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","22  2.784332e-08    3.840922e-12   0.065691    7.952294e-07   \n","23  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","24  2.729167e-08    4.988742e-12   0.064389    9.026103e-07   \n","25  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","26  1.451916e-05    4.613413e-09   0.074338    3.635621e-05   \n","27  0.000000e+00    0.000000e+00   0.000000    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0         6.732343e-08       0.265801      True  \n","1         0.000000e+00       0.000000     False  \n","2         3.663408e-09       0.929321      True  \n","3         0.000000e+00       0.000000     False  \n","4         1.476646e-09       1.172905      True  \n","5         0.000000e+00       0.000000     False  \n","6         4.596262e-10       1.148610      True  \n","7         0.000000e+00       0.000000     False  \n","8         1.794783e-10       1.441105      True  \n","9         0.000000e+00       0.000000     False  \n","10        4.525161e-11       1.451789      True  \n","11        0.000000e+00       0.000000     False  \n","12        2.785579e-11       1.240944      True  \n","13        0.000000e+00       0.000000     False  \n","14        1.031113e-11       1.681883      True  \n","15        0.000000e+00       0.000000     False  \n","16        2.898821e-12       1.881961      True  \n","17        0.000000e+00       0.000000     False  \n","18        2.293842e-12       1.871452      True  \n","19        0.000000e+00       0.000000     False  \n","20        2.548202e-12       1.871061      True  \n","21        0.000000e+00       0.000000     False  \n","22        3.209310e-12       1.876181      True  \n","23        0.000000e+00       0.000000     False  \n","24        4.174782e-12       2.129525      True  \n","25        0.000000e+00       0.000000     False  \n","26        3.502445e-09       0.186144      True  \n","27        0.000000e+00       0.000000     False  \n","Parameter Sparsity: 151390/14719818 (0.0103)\n","FLOP Sparsity: 18714118/313478154 (0.0597)\n","Saving results.\n"]}]},{"cell_type":"markdown","source":["SynFlow"],"metadata":{"id":"VXJOjslDrlxr"}},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --post-epochs 10 --compression 0.05 --expid synflow_0_05"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQSNy096rnkA","executionInfo":{"status":"ok","timestamp":1677470274720,"user_tz":300,"elapsed":277413,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"aaee3780-98e9-4932-880e-4c3f78ed5d98"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7061300330005906\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.91it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:25<00:00, 26.58s/it]\n","Time_post_train:  267.41743061000034\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.414332          11.66          50.38\n","Final      10    0.593267   0.621710          78.82          98.60\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.998264    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.996460    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.993198   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.986152   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.972612   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.972772   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.944177  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.888635  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.888531  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.865655  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.866078  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.856663  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.999609     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance     score sum  score abs mean  \\\n","0   1.704298e+19             inf  2.945027e+22    1.704298e+19   \n","1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","2   7.988898e+17             inf  2.945027e+22    7.988898e+17   \n","3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","4   3.994448e+17             inf  2.945027e+22    3.994448e+17   \n","5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","6   1.997224e+17             inf  2.945027e+22    1.997224e+17   \n","7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","8   9.986122e+16             inf  2.945027e+22    9.986122e+16   \n","9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","10  4.993060e+16             inf  2.945027e+22    4.993060e+16   \n","11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","12  4.993061e+16             inf  2.945027e+22    4.993061e+16   \n","13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","14  2.496530e+16             inf  2.945027e+22    2.496530e+16   \n","15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   \n","17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   \n","19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","20  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","22  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","24  1.248265e+16             inf  2.945028e+22    1.248265e+16   \n","25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","26  5.752006e+18             inf  2.945027e+22    5.752006e+18   \n","27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0                  inf   2.945027e+22      True  \n","1         0.000000e+00   0.000000e+00     False  \n","2                  inf   2.945027e+22      True  \n","3         0.000000e+00   0.000000e+00     False  \n","4                  inf   2.945027e+22      True  \n","5         0.000000e+00   0.000000e+00     False  \n","6                  inf   2.945027e+22      True  \n","7         0.000000e+00   0.000000e+00     False  \n","8                  inf   2.945027e+22      True  \n","9         0.000000e+00   0.000000e+00     False  \n","10                 inf   2.945027e+22      True  \n","11        0.000000e+00   0.000000e+00     False  \n","12                 inf   2.945027e+22      True  \n","13        0.000000e+00   0.000000e+00     False  \n","14                 inf   2.945027e+22      True  \n","15        0.000000e+00   0.000000e+00     False  \n","16        9.402646e+31   2.945026e+22      True  \n","17        0.000000e+00   0.000000e+00     False  \n","18        9.415211e+31   2.945027e+22      True  \n","19        0.000000e+00   0.000000e+00     False  \n","20                 inf   2.945026e+22      True  \n","21        0.000000e+00   0.000000e+00     False  \n","22                 inf   2.945026e+22      True  \n","23        0.000000e+00   0.000000e+00     False  \n","24                 inf   2.945028e+22      True  \n","25        0.000000e+00   0.000000e+00     False  \n","26                 inf   2.945027e+22      True  \n","27        0.000000e+00   0.000000e+00     False  \n","Parameter Sparsity: 13119512/14719818 (0.8913)\n","FLOP Sparsity: 297416253/313478154 (0.9488)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --post-epochs 10 --compression 0.1 --expid synflow_0_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQmzFH7prtD2","executionInfo":{"status":"ok","timestamp":1677470552855,"user_tz":300,"elapsed":278154,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"2f62a32a-9059-4869-c5b7-4bf7cef91e69"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7174773970000388\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.99it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:25<00:00, 26.56s/it]\n","Time_post_train:  267.2618034400002\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.403514          11.46          49.92\n","Final      10    0.585857   0.623776          79.25          98.27\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.999421     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.996582    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.993096    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.986925   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.974199   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.947367   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.947562   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.892713  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.788353  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.787956  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.746524  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.746590  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.731970  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.999609     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance     score sum  score abs mean  \\\n","0   1.704298e+19             inf  2.945027e+22    1.704298e+19   \n","1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","2   7.988898e+17             inf  2.945027e+22    7.988898e+17   \n","3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","4   3.994448e+17             inf  2.945027e+22    3.994448e+17   \n","5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","6   1.997224e+17             inf  2.945027e+22    1.997224e+17   \n","7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","8   9.986122e+16             inf  2.945027e+22    9.986122e+16   \n","9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","10  4.993060e+16             inf  2.945027e+22    4.993060e+16   \n","11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","12  4.993061e+16             inf  2.945027e+22    4.993061e+16   \n","13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","14  2.496530e+16             inf  2.945027e+22    2.496530e+16   \n","15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   \n","17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   \n","19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","20  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","22  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","24  1.248265e+16             inf  2.945028e+22    1.248265e+16   \n","25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","26  5.752006e+18             inf  2.945027e+22    5.752006e+18   \n","27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0                  inf   2.945027e+22      True  \n","1         0.000000e+00   0.000000e+00     False  \n","2                  inf   2.945027e+22      True  \n","3         0.000000e+00   0.000000e+00     False  \n","4                  inf   2.945027e+22      True  \n","5         0.000000e+00   0.000000e+00     False  \n","6                  inf   2.945027e+22      True  \n","7         0.000000e+00   0.000000e+00     False  \n","8                  inf   2.945027e+22      True  \n","9         0.000000e+00   0.000000e+00     False  \n","10                 inf   2.945027e+22      True  \n","11        0.000000e+00   0.000000e+00     False  \n","12                 inf   2.945027e+22      True  \n","13        0.000000e+00   0.000000e+00     False  \n","14                 inf   2.945027e+22      True  \n","15        0.000000e+00   0.000000e+00     False  \n","16        9.402646e+31   2.945026e+22      True  \n","17        0.000000e+00   0.000000e+00     False  \n","18        9.415211e+31   2.945027e+22      True  \n","19        0.000000e+00   0.000000e+00     False  \n","20                 inf   2.945026e+22      True  \n","21        0.000000e+00   0.000000e+00     False  \n","22                 inf   2.945026e+22      True  \n","23        0.000000e+00   0.000000e+00     False  \n","24                 inf   2.945028e+22      True  \n","25        0.000000e+00   0.000000e+00     False  \n","26                 inf   2.945027e+22      True  \n","27        0.000000e+00   0.000000e+00     False  \n","Parameter Sparsity: 11693237/14719818 (0.7944)\n","FLOP Sparsity: 282939167/313478154 (0.9026)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --post-epochs 10 --compression 0.2 --expid synflow_0_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFkpru3PrtcC","executionInfo":{"status":"ok","timestamp":1677470831123,"user_tz":300,"elapsed":278278,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"29ae0128-6bf1-45b2-f180-52ac249fb972"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7010440140002174\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.96it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:25<00:00, 26.58s/it]\n","Time_post_train:  267.37011771700054\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.396131          10.05          51.06\n","Final      10    0.561505   0.608203          79.55          98.42\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.998843     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.993490    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.987115    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.974752   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.950578   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.900650   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.900650   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.799322  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.611696  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.611219  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.549993  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.550156  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.533391  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.999219     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance     score sum  score abs mean  \\\n","0   1.704298e+19             inf  2.945027e+22    1.704298e+19   \n","1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","2   7.988898e+17             inf  2.945027e+22    7.988898e+17   \n","3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","4   3.994448e+17             inf  2.945027e+22    3.994448e+17   \n","5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","6   1.997224e+17             inf  2.945027e+22    1.997224e+17   \n","7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","8   9.986122e+16             inf  2.945027e+22    9.986122e+16   \n","9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","10  4.993060e+16             inf  2.945027e+22    4.993060e+16   \n","11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","12  4.993061e+16             inf  2.945027e+22    4.993061e+16   \n","13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","14  2.496530e+16             inf  2.945027e+22    2.496530e+16   \n","15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   \n","17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   \n","19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","20  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","22  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","24  1.248265e+16             inf  2.945028e+22    1.248265e+16   \n","25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","26  5.752006e+18             inf  2.945027e+22    5.752006e+18   \n","27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0                  inf   2.945027e+22      True  \n","1         0.000000e+00   0.000000e+00     False  \n","2                  inf   2.945027e+22      True  \n","3         0.000000e+00   0.000000e+00     False  \n","4                  inf   2.945027e+22      True  \n","5         0.000000e+00   0.000000e+00     False  \n","6                  inf   2.945027e+22      True  \n","7         0.000000e+00   0.000000e+00     False  \n","8                  inf   2.945027e+22      True  \n","9         0.000000e+00   0.000000e+00     False  \n","10                 inf   2.945027e+22      True  \n","11        0.000000e+00   0.000000e+00     False  \n","12                 inf   2.945027e+22      True  \n","13        0.000000e+00   0.000000e+00     False  \n","14                 inf   2.945027e+22      True  \n","15        0.000000e+00   0.000000e+00     False  \n","16        9.402646e+31   2.945026e+22      True  \n","17        0.000000e+00   0.000000e+00     False  \n","18        9.415211e+31   2.945027e+22      True  \n","19        0.000000e+00   0.000000e+00     False  \n","20                 inf   2.945026e+22      True  \n","21        0.000000e+00   0.000000e+00     False  \n","22                 inf   2.945026e+22      True  \n","23        0.000000e+00   0.000000e+00     False  \n","24                 inf   2.945028e+22      True  \n","25        0.000000e+00   0.000000e+00     False  \n","26                 inf   2.945027e+22      True  \n","27        0.000000e+00   0.000000e+00     False  \n","Parameter Sparsity: 9289139/14719818 (0.6311)\n","FLOP Sparsity: 257583398/313478154 (0.8217)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --post-epochs 10 --compression 0.5 --expid synflow_0_5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfRY3AvQrt2l","executionInfo":{"status":"ok","timestamp":1677471109764,"user_tz":300,"elapsed":278664,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"a5f538ed-a4e1-4c41-d4e9-dc735e7a8e0a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7023809359998268\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.97it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:25<00:00, 26.59s/it]\n","Time_post_train:  267.5436757509997\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.329105           9.99          51.18\n","Final      10    0.479727   0.597574          80.24          98.70\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.997685     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.983805    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.969618    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.940477   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.881911   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.766754   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.766447   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.546590  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.232446  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.232694  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.211462  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.211558  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.210058  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.997656     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance     score sum  score abs mean  \\\n","0   1.704298e+19             inf  2.945027e+22    1.704298e+19   \n","1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","2   7.988898e+17             inf  2.945027e+22    7.988898e+17   \n","3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","4   3.994448e+17             inf  2.945027e+22    3.994448e+17   \n","5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","6   1.997224e+17             inf  2.945027e+22    1.997224e+17   \n","7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","8   9.986122e+16             inf  2.945027e+22    9.986122e+16   \n","9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","10  4.993060e+16             inf  2.945027e+22    4.993060e+16   \n","11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","12  4.993061e+16             inf  2.945027e+22    4.993061e+16   \n","13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","14  2.496530e+16             inf  2.945027e+22    2.496530e+16   \n","15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   \n","17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   \n","19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","20  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","22  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","24  1.248265e+16             inf  2.945028e+22    1.248265e+16   \n","25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","26  5.752006e+18             inf  2.945027e+22    5.752006e+18   \n","27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0                  inf   2.945027e+22      True  \n","1         0.000000e+00   0.000000e+00     False  \n","2                  inf   2.945027e+22      True  \n","3         0.000000e+00   0.000000e+00     False  \n","4                  inf   2.945027e+22      True  \n","5         0.000000e+00   0.000000e+00     False  \n","6                  inf   2.945027e+22      True  \n","7         0.000000e+00   0.000000e+00     False  \n","8                  inf   2.945027e+22      True  \n","9         0.000000e+00   0.000000e+00     False  \n","10                 inf   2.945027e+22      True  \n","11        0.000000e+00   0.000000e+00     False  \n","12                 inf   2.945027e+22      True  \n","13        0.000000e+00   0.000000e+00     False  \n","14                 inf   2.945027e+22      True  \n","15        0.000000e+00   0.000000e+00     False  \n","16        9.402646e+31   2.945026e+22      True  \n","17        0.000000e+00   0.000000e+00     False  \n","18        9.415211e+31   2.945027e+22      True  \n","19        0.000000e+00   0.000000e+00     False  \n","20                 inf   2.945026e+22      True  \n","21        0.000000e+00   0.000000e+00     False  \n","22                 inf   2.945026e+22      True  \n","23        0.000000e+00   0.000000e+00     False  \n","24                 inf   2.945028e+22      True  \n","25        0.000000e+00   0.000000e+00     False  \n","26                 inf   2.945027e+22      True  \n","27        0.000000e+00   0.000000e+00     False  \n","Parameter Sparsity: 4657711/14719818 (0.3164)\n","FLOP Sparsity: 201358535/313478154 (0.6423)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --post-epochs 10 --compression 1 --expid synflow_1_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJwkblaWruLf","executionInfo":{"status":"ok","timestamp":1677471388583,"user_tz":300,"elapsed":278839,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"483d8fa9-df45-4624-8939-5bf06f0410a7"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.7217927459996645\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.88it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:26<00:00, 26.63s/it]\n","Time_post_train:  267.9027391519994\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302601          10.00          49.94\n","Final      10    0.500457   0.551291          81.39          99.01\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.997106     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.964790    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.928589    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.859741   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.726661   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.484929   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.484746   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.162064  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.007960  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.007958  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.029585  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.029672  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.033553  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.994922     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance     score sum  score abs mean  \\\n","0   1.704298e+19             inf  2.945027e+22    1.704298e+19   \n","1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","2   7.988898e+17             inf  2.945027e+22    7.988898e+17   \n","3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","4   3.994448e+17             inf  2.945027e+22    3.994448e+17   \n","5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","6   1.997224e+17             inf  2.945027e+22    1.997224e+17   \n","7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","8   9.986122e+16             inf  2.945027e+22    9.986122e+16   \n","9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","10  4.993060e+16             inf  2.945027e+22    4.993060e+16   \n","11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","12  4.993061e+16             inf  2.945027e+22    4.993061e+16   \n","13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","14  2.496530e+16             inf  2.945027e+22    2.496530e+16   \n","15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   \n","17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   \n","19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","20  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","22  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","24  1.248265e+16             inf  2.945028e+22    1.248265e+16   \n","25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","26  5.752006e+18             inf  2.945027e+22    5.752006e+18   \n","27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0                  inf   2.945027e+22      True  \n","1         0.000000e+00   0.000000e+00     False  \n","2                  inf   2.945027e+22      True  \n","3         0.000000e+00   0.000000e+00     False  \n","4                  inf   2.945027e+22      True  \n","5         0.000000e+00   0.000000e+00     False  \n","6                  inf   2.945027e+22      True  \n","7         0.000000e+00   0.000000e+00     False  \n","8                  inf   2.945027e+22      True  \n","9         0.000000e+00   0.000000e+00     False  \n","10                 inf   2.945027e+22      True  \n","11        0.000000e+00   0.000000e+00     False  \n","12                 inf   2.945027e+22      True  \n","13        0.000000e+00   0.000000e+00     False  \n","14                 inf   2.945027e+22      True  \n","15        0.000000e+00   0.000000e+00     False  \n","16        9.402646e+31   2.945026e+22      True  \n","17        0.000000e+00   0.000000e+00     False  \n","18        9.415211e+31   2.945027e+22      True  \n","19        0.000000e+00   0.000000e+00     False  \n","20                 inf   2.945026e+22      True  \n","21        0.000000e+00   0.000000e+00     False  \n","22                 inf   2.945026e+22      True  \n","23        0.000000e+00   0.000000e+00     False  \n","24                 inf   2.945028e+22      True  \n","25        0.000000e+00   0.000000e+00     False  \n","26                 inf   2.945027e+22      True  \n","27        0.000000e+00   0.000000e+00     False  \n","Parameter Sparsity: 1475793/14719818 (0.1003)\n","FLOP Sparsity: 143301085/313478154 (0.4571)\n","Saving results.\n"]}]},{"cell_type":"code","source":["!python main.py --model-class lottery --model vgg16 --dataset cifar10 --experiment singleshot --pruner synflow --post-epochs 10 --compression 2 --expid synflow_2_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJ7z7nOfrugM","executionInfo":{"status":"ok","timestamp":1677471651360,"user_tz":300,"elapsed":262783,"user":{"displayName":"Xiaxin Shen","userId":"02942036663319565536"}},"outputId":"c0451177-fe20-4643-98d0-5e39c15198af"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cifar10 dataset.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Creating lottery-vgg16 model.\n","Pre-Train for 0 epochs.\n","0it [00:00, ?it/s]\n","Time_pre_train:  1.689628459999767\n","Pruning with synflow for 1 epochs.\n","100% 1/1 [00:00<00:00,  2.75it/s]\n","Post-Training for 10 epochs.\n","100% 10/10 [04:09<00:00, 24.91s/it]\n","Time_post_train:  250.67259388000002\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n","Train results:\n","                train_loss  test_loss  top1_accuracy  top5_accuracy\n","Init.      0          NaN   2.417717          11.73          50.17\n","Pre-Prune  0          NaN   2.417717          11.73          50.17\n","Post-Prune 0          NaN   2.302585          10.00          50.00\n","Final      10    2.302652   2.302590          10.00          50.00\n","Prune results:\n","             module   param  sparsity     size             shape     flops  \\\n","0    layers.0.conv  weight  0.988426     1728     (64, 3, 3, 3)   1769472   \n","1    layers.0.conv    bias  1.000000       64             (64,)     65536   \n","2    layers.1.conv  weight  0.806396    36864    (64, 64, 3, 3)  37748736   \n","3    layers.1.conv    bias  1.000000       64             (64,)     65536   \n","4    layers.3.conv  weight  0.626017    73728   (128, 64, 3, 3)  18874368   \n","5    layers.3.conv    bias  1.000000      128            (128,)     32768   \n","6    layers.4.conv  weight  0.332425   147456  (128, 128, 3, 3)  37748736   \n","7    layers.4.conv    bias  1.000000      128            (128,)     32768   \n","8    layers.6.conv  weight  0.052199   294912  (256, 128, 3, 3)  18874368   \n","9    layers.6.conv    bias  1.000000      256            (256,)     16384   \n","10   layers.7.conv  weight  0.000154   589824  (256, 256, 3, 3)  37748736   \n","11   layers.7.conv    bias  1.000000      256            (256,)     16384   \n","12   layers.8.conv  weight  0.000112   589824  (256, 256, 3, 3)  37748736   \n","13   layers.8.conv    bias  1.000000      256            (256,)     16384   \n","14  layers.10.conv  weight  0.000000  1179648  (512, 256, 3, 3)  18874368   \n","15  layers.10.conv    bias  1.000000      512            (512,)      8192   \n","16  layers.11.conv  weight  0.000000  2359296  (512, 512, 3, 3)  37748736   \n","17  layers.11.conv    bias  1.000000      512            (512,)      8192   \n","18  layers.12.conv  weight  0.000000  2359296  (512, 512, 3, 3)  37748736   \n","19  layers.12.conv    bias  1.000000      512            (512,)      8192   \n","20  layers.14.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184   \n","21  layers.14.conv    bias  1.000000      512            (512,)      2048   \n","22  layers.15.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184   \n","23  layers.15.conv    bias  1.000000      512            (512,)      2048   \n","24  layers.16.conv  weight  0.000000  2359296  (512, 512, 3, 3)   9437184   \n","25  layers.16.conv    bias  1.000000      512            (512,)      2048   \n","26              fc  weight  0.975977     5120         (10, 512)      5120   \n","27              fc    bias  1.000000       10             (10,)        10   \n","\n","      score mean  score variance     score sum  score abs mean  \\\n","0   1.704298e+19             inf  2.945027e+22    1.704298e+19   \n","1   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","2   7.988898e+17             inf  2.945027e+22    7.988898e+17   \n","3   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","4   3.994448e+17             inf  2.945027e+22    3.994448e+17   \n","5   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","6   1.997224e+17             inf  2.945027e+22    1.997224e+17   \n","7   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","8   9.986122e+16             inf  2.945027e+22    9.986122e+16   \n","9   0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","10  4.993060e+16             inf  2.945027e+22    4.993060e+16   \n","11  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","12  4.993061e+16             inf  2.945027e+22    4.993061e+16   \n","13  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","14  2.496530e+16             inf  2.945027e+22    2.496530e+16   \n","15  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","16  1.248265e+16    9.402646e+31  2.945026e+22    1.248265e+16   \n","17  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","18  1.248265e+16    9.415211e+31  2.945027e+22    1.248265e+16   \n","19  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","20  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","21  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","22  1.248265e+16             inf  2.945026e+22    1.248265e+16   \n","23  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","24  1.248265e+16             inf  2.945028e+22    1.248265e+16   \n","25  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","26  5.752006e+18             inf  2.945027e+22    5.752006e+18   \n","27  0.000000e+00    0.000000e+00  0.000000e+00    0.000000e+00   \n","\n","    score abs variance  score abs sum  prunable  \n","0                  inf   2.945027e+22      True  \n","1         0.000000e+00   0.000000e+00     False  \n","2                  inf   2.945027e+22      True  \n","3         0.000000e+00   0.000000e+00     False  \n","4                  inf   2.945027e+22      True  \n","5         0.000000e+00   0.000000e+00     False  \n","6                  inf   2.945027e+22      True  \n","7         0.000000e+00   0.000000e+00     False  \n","8                  inf   2.945027e+22      True  \n","9         0.000000e+00   0.000000e+00     False  \n","10                 inf   2.945027e+22      True  \n","11        0.000000e+00   0.000000e+00     False  \n","12                 inf   2.945027e+22      True  \n","13        0.000000e+00   0.000000e+00     False  \n","14                 inf   2.945027e+22      True  \n","15        0.000000e+00   0.000000e+00     False  \n","16        9.402646e+31   2.945026e+22      True  \n","17        0.000000e+00   0.000000e+00     False  \n","18        9.415211e+31   2.945027e+22      True  \n","19        0.000000e+00   0.000000e+00     False  \n","20                 inf   2.945026e+22      True  \n","21        0.000000e+00   0.000000e+00     False  \n","22                 inf   2.945026e+22      True  \n","23        0.000000e+00   0.000000e+00     False  \n","24                 inf   2.945028e+22      True  \n","25        0.000000e+00   0.000000e+00     False  \n","26                 inf   2.945027e+22      True  \n","27        0.000000e+00   0.000000e+00     False  \n","Parameter Sparsity: 151390/14719818 (0.0103)\n","FLOP Sparsity: 57830479/313478154 (0.1845)\n","Saving results.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"m6vFdH5csn-9"},"execution_count":null,"outputs":[]}]}